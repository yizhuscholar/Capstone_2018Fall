{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hourly Data #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define data location\n",
    "#location = '/Users/mithras/Documents/_SCHOOL/_Drexel/BUSN 710 - Capstone/Data/Forecasting Project/'\n",
    "location = '/Users/loki/Documents/Data/Forecasting Project/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning & Checking "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hourly data\n",
    "use_jan_in = pd.read_excel(location+'Zip_HourlylUsage_2018.01.xlsx')\n",
    "use_feb_in = pd.read_excel(location+'PECO Zip HourlyUsage_2018.02.xlsx')\n",
    "use_mar_in = pd.read_excel(location+'PECO Zip HourlyUsage_2018.03.xlsx')\n",
    "use_apr_in = pd.read_excel(location+'PECO Zip HourlyUsage_2018.04.xlsx')\n",
    "use_may_in = pd.read_excel(location+'PECO Zip HourlyUsage_2018.05.xlsx')\n",
    "use_jun_in = pd.read_excel(location+'PECO Zip HourlyUsage_2018.06.xlsx')\n",
    "use_jul_in = pd.read_excel(location+'Zip_HourlylUsage_2018.07.xlsx')\n",
    "use_aug_in = pd.read_excel(location+'PECO Zip HourlyUsage_2018.08.xlsx')\n",
    "use_sep_in = pd.read_excel(location+'PECO Zip HourlyUsage_2018.09.xlsx')\n",
    "use_oct_in = pd.read_excel(location+'PECO Zip HourlyUsage_2017.10.xlsx')\n",
    "use_nov_in = pd.read_excel(location+'PECO Zip HourlyUsage_2017.11.xlsx') \n",
    "use_dec_in = pd.read_excel(location+'PECO Zip HourlyUsage_2017.12.xlsx') \n",
    "\n",
    "#weather data\n",
    "temp_in = pd.read_excel(location+'Weather Data for Drexel 9_28_2018.xlsx', sheet_name=\"TMP\")\n",
    "humid_in = pd.read_excel(location+'Weather Data for Drexel 9_28_2018.xlsx', sheet_name=\"HUM\")\n",
    "wind_in = pd.read_excel(location+'Weather Data for Drexel 9_28_2018.xlsx', sheet_name=\"WSP\")\n",
    "cloud_in = pd.read_excel(location+'Weather Data for Drexel 9_28_2018.xlsx', sheet_name=\"CC\")\n",
    "\n",
    "# other data\n",
    "customer_in = pd.read_excel(location+'PECO Zip Customer 2018.10.01 v2.xlsx', sheet_name=\"Account\")\n",
    "ratecode_in = pd.read_excel(location+'Rate Codes for Drexel 9_28_2018.xlsx')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge use data\n",
    "hourly_in = [use_oct_in, use_nov_in, use_dec_in, use_jan_in, use_feb_in, use_mar_in, \n",
    "             use_apr_in, use_may_in, use_jun_in, use_jul_in, use_aug_in, use_sep_in]\n",
    "use = pd.concat(hourly_in)\n",
    "\n",
    "#hourly = hourly.rename(columns={'DAccountID':'DACCOUNTID', 'DMeterNo':'DMETERNO',\n",
    "#                              'DAILY_INTERVAL_USAGE':'Use'})\n",
    "\n",
    "# list(hourly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify unique ids/meterno pairs\n",
    "def uniqueIDs(dfs, cols):\n",
    "    \"\"\"\n",
    "    Identifies uniques in specified cols and intersects across dfs to return only the overlap\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dfs : list of pandas data frames with consistent headers\n",
    "    cols : list of column names to track\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    ids : pandas df with cols columns containing only ids present in all dfs\n",
    "    \"\"\"\n",
    "    \n",
    "    dfs2 = []\n",
    "    for df in dfs:\n",
    "        dfs2.append(df.loc[:, df.columns.isin(cols)].drop_duplicates())\n",
    "   \n",
    "    ids = dfs2[0]\n",
    "    for i in range(1,len(dfs2)):\n",
    "        ids = pd.merge(ids, dfs2[i], how='inner')\n",
    "        \n",
    "    return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify unique ids/meterno pairs\n",
    "ids = uniqueIDs(hourly_in,['DACCOUNTID','DMETERNO'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean\n",
    "use = use.loc[use['UOM'] == 'CCF']\n",
    "use = use.drop(columns=['UOM'])\n",
    "\n",
    "customer = customer_in.loc[customer_in['FUELTYPE'] == 'GAS']\n",
    "customer = customer.drop(columns=['CITY', 'STATE', 'ZIPCODE', 'FUELTYPE', 'COUNTYCODE'])\n",
    "\n",
    "temp = temp_in.drop(columns=['Avg','HighDB','LowDB','AvgHL','Gas Day Average','HDD-HL','CDD-HL','HDD-24','CDD-24',\n",
    "                            'Unnamed: 34','Unnamed: 35'])\n",
    "humid = humid_in.drop(columns=['Avg','Unnamed: 26','Unnamed: 27','Unnamed: 28'])\n",
    "wind = wind_in.drop(columns=['Avg','Unnamed: 26'])\n",
    "cloud = cloud_in.drop(columns=['AvgDaytime','Avg'])\n",
    "\n",
    "# convert to datetime\n",
    "use['Dt'] =  pd.to_datetime(use['METERREADDATE'])\n",
    "temp['Dt'] =  pd.to_datetime(temp['Dt'])\n",
    "humid['Dt'] =  pd.to_datetime(humid['Dt'])\n",
    "wind['Dt'] =  pd.to_datetime(wind['Dt'])\n",
    "cloud['Dt'] =  pd.to_datetime(cloud['Dt'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Restructuring ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions for naming consistency\n",
    "def decrement(x, startswith, split):\n",
    "    \"\"\"\n",
    "    decrements a passed string of form \"demo#\" by 1\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x : string to be decremented\n",
    "    split : string to split on\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    y : decremented string\n",
    "    \"\"\"\n",
    "    if x.startswith(startswith):\n",
    "        a,b = x.split(split)\n",
    "        b = int(b)-1\n",
    "        y = a + split + str(b)\n",
    "\n",
    "        return y\n",
    "\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "    \n",
    "def interval_to_hour(df):\n",
    "    \"\"\"\n",
    "    function for fast rename/relabel during tidying process\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas data frame\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    df : data frame with updated column names\n",
    "    \"\"\"\n",
    "    \n",
    "    df = df.rename(columns=lambda x: decrement(x, \"INTERVAL_\", \"_\"))\n",
    "    df = df.rename(columns=lambda x: x.replace(\"INTERVAL_\", \"HR\"))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename for consistency\n",
    "use = interval_to_hour(use)\n",
    "use = use.drop(columns=['METERREADDATE','HR24'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tidy / Stack data (transform into tall data - one row per customer per hour):\n",
    "# ref: http://www.jeannicholashould.com/tidy-data-in-python.html\n",
    "tidy_use = pd.melt(use, \n",
    "                   id_vars=['DACCOUNTID','DMETERNO','Dt'],\n",
    "                   var_name='Hour', value_name='Use')\n",
    "\n",
    "tidy_temp = pd.melt(temp, \n",
    "                    id_vars=['Dt'],\n",
    "                    var_name='Hour', value_name='Temp')\n",
    "\n",
    "tidy_humid = pd.melt(temp,\n",
    "                     id_vars=['Dt'],\n",
    "                     var_name='Hour', value_name='Humid')\n",
    "\n",
    "tidy_wind = pd.melt(temp, \n",
    "                    id_vars=['Dt'],\n",
    "                    var_name='Hour', value_name='Wind')\n",
    "\n",
    "tidy_cloud = pd.melt(temp, \n",
    "                     id_vars=['Dt'],\n",
    "                     var_name='Hour', value_name='Cloud')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# relabel & retype for sorting\n",
    "tidy_use['Hour'] = tidy_use['Hour'].str.extract('(\\d+)').astype(int)\n",
    "tidy_temp['Hour'] = tidy_temp['Hour'].str.extract('(\\d+)').astype(int)\n",
    "tidy_humid['Hour'] = tidy_humid['Hour'].str.extract('(\\d+)').astype(int)\n",
    "tidy_wind['Hour'] = tidy_wind['Hour'].str.extract('(\\d+)').astype(int)\n",
    "tidy_cloud['Hour'] = tidy_cloud['Hour'].str.extract('(\\d+)').astype(int)\n",
    "\n",
    "# sort by date & time\n",
    "tidy_use = tidy_use.sort_values(by=[\"Dt\",\"Hour\"])\n",
    "tidy_temp = tidy_temp.sort_values(by=[\"Dt\",\"Hour\"])\n",
    "tidy_humid = tidy_humid.sort_values(by=[\"Dt\",\"Hour\"])\n",
    "tidy_wind = tidy_wind.sort_values(by=[\"Dt\",\"Hour\"])\n",
    "tidy_cloud = tidy_cloud.sort_values(by=[\"Dt\",\"Hour\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lagDeltas(data, colname, newname, lag):\n",
    "    \"\"\"Calculates specified lag of provided column and saves as a new column\n",
    "     \n",
    "    Parameters\n",
    "    ----------\n",
    "    data : pandas dataframe object to be used calculate lag 0 and lag 1 deltas\n",
    "    colname : name of column to be lagged\n",
    "    newname : name of result to be added to pandas dataframe\n",
    "    lag : number of lags to take \n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    lagData : pandas dataframe with new column\n",
    "    \"\"\"\n",
    "    \n",
    "    data[newname] = data[colname].shift(lag)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find lags of weather variables\n",
    "# Lag temp\n",
    "for i in range(1,7):\n",
    "    temp = lagDeltas(tidy_temp, \"Temp\", \"Temp\"+str(i), i)\n",
    "\n",
    "# Lag humid\n",
    "for i in range(1,7):\n",
    "    humid = lagDeltas(tidy_humid, \"Humid\", \"Humid\"+str(i), i)\n",
    "    \n",
    "# Lag wind\n",
    "for i in range(1,7):\n",
    "    wind = lagDeltas(tidy_wind, \"Wind\", \"Wind\"+str(i), i)\n",
    "\n",
    "# Lag cloud\n",
    "for i in range(1,7):\n",
    "    cloud = lagDeltas(tidy_cloud, \"Cloud\", \"Cloud\"+str(i), i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge\n",
    "weather1 = pd.merge(temp, humid, how='inner', on=['Dt', 'Hour'])\n",
    "weather2 = pd.merge(wind, cloud, how='inner', on=['Dt', 'Hour'])\n",
    "weather = pd.merge(weather1, weather2, how='inner', on=['Dt', 'Hour'])\n",
    "\n",
    "hourly = pd.merge(tidy_use, weather, how='inner', on=['Dt', 'Hour'])\n",
    "# CustIDs || Date | Time | Consumption |||| Weather_variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add dummy variables for day-of-week and holidays\n",
    "hourly = hourly.join(pd.get_dummies(hourly['Dt'].dt.weekday_name))\n",
    "\n",
    "from pandas.tseries.holiday import USFederalHolidayCalendar\n",
    "cal = USFederalHolidayCalendar()\n",
    "dr = pd.date_range(start='2010-01-01', end='2018-12-31')\n",
    "holidays = cal.holidays(start=dr.min(), end=dr.max())\n",
    "\n",
    "hourly = hourly.join(pd.get_dummies(hourly['Dt'].isin(holidays)))\n",
    "hourly = hourly.rename(columns={True:'Holiday'})\n",
    "hourly = hourly.drop(columns=[False])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only records with full year of data\n",
    "hourly = hourly.merge(ids, how='inner', on=['DACCOUNTID','DMETERNO'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13806\n",
      "13877\n",
      "There is 99.48836203790445 percent overlap between customer rate code info and gas customer use info\n"
     ]
    }
   ],
   "source": [
    "# cross-reference ids with sufficient data against rate code list\n",
    "# rate_codes = ids.merge(customer, how='inner', on=['DACCOUNTID','DMETERNO'])\n",
    "\n",
    "# we need to use customer_in instead of customer because customer filters out electric rate codes, some of whom still use gas\n",
    "rate_codes = ids.merge(customer_in, how='inner', on=['DACCOUNTID','DMETERNO'])\n",
    "\n",
    "print(len(ids))\n",
    "print(len(rate_codes))\n",
    "print(f\"There is {100*len(ids)/len(rate_codes)} percent overlap between customer rate code info and gas customer use info\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save file(s) as pickles\n",
    "# using save location to save with other data files outside of git repo\n",
    "# data.to_csv(location+'peco.csv', sep='\\t')\n",
    "hourly.to_pickle(location+'peco_hourly.pkl.zip') \n",
    "ids.to_pickle(location+'peco_hourly_ids.pkl.zip')\n",
    "rate_codes.to_pickle(location+'peco_hourly_rate_codes.pkl.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hourly Data #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from timeit import default_timer as timer\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define data location\n",
    "#location = '/Users/mithras/Documents/_SCHOOL/_Drexel/BUSN 710 - Capstone/Data/Forecasting Project/'\n",
    "location = '/Users/loki/Documents/Data/Forecasting Project/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning & Checking "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hourly data\n",
    "use_jan_in = pd.read_excel(location+'Zip_HourlylUsage_2018.01.xlsx')\n",
    "use_jul_in = pd.read_excel(location+'Zip_HourlylUsage_2018.07.xlsx')\n",
    "\n",
    "# daily data\n",
    "use_oct_in = pd.read_excel(location+'Sample Usage_2017.10 Oct.xlsx')\n",
    "use_nov_in = pd.read_excel(location+'Sample Usage_2017.11 Nov.xlsx')\n",
    "use_dec_in = pd.read_excel(location+'Sample Usage_2017.12 Dec.xlsx')\n",
    "# use_jan_in = pd.read_excel(location+'Sample Usage_2018.01 Jan.xlsx')\n",
    "use_feb_in = pd.read_excel(location+'Sample Usage_2018.02 Feb.xlsx')\n",
    "use_mar_in = pd.read_excel(location+'Sample Usage_2018.03 March.xlsx')\n",
    "use_apr_in = pd.read_excel(location+'Sample Usage_2018.04 April.xlsx')\n",
    "use_may_in = pd.read_excel(location+'Sample Usage_2018.05 May.xlsx')\n",
    "use_jun_in = pd.read_excel(location+'Sample Usage_2018.06 June.xlsx')\n",
    "# use_jul_in = pd.read_excel(location+'Sample Usage_2018.07 Jul.xlsx')\n",
    "use_aug_in = pd.read_excel(location+'Sample Usage_2018.08 Aug.xlsx')\n",
    "use_sep_in = pd.read_excel(location+'Sample Usage_2018.09 Sep.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge use data\n",
    "daily_in = [use_oct_in, use_nov_in, use_dec_in, use_feb_in, use_mar_in, \n",
    "            use_apr_in, use_may_in, use_jun_in, use_aug_in, use_sep_in]\n",
    "daily = pd.concat(daily_in)\n",
    "daily = daily.rename(columns={'DAccountID':'DACCOUNTID', 'DMeterNo':'DMETERNO',\n",
    "                              'DAILY_INTERVAL_USAGE':'Use'})\n",
    "\n",
    "hourly_in = [use_jan_in, use_jul_in]\n",
    "hourly = pd.concat(hourly_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del (use_oct_in, use_nov_in, use_dec_in, use_feb_in, use_mar_in, \n",
    "     use_apr_in, use_may_in, use_jun_in, use_aug_in, use_sep_in, \n",
    "     use_jan_in, use_jul_in, daily_in, hourly_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create ID tuple\n",
    "ids = pd.Series(list(map(tuple, daily[['DACCOUNTID', 'DMETERNO']].values)))\n",
    "daily['ID'] = ids.values\n",
    "\n",
    "ids = pd.Series(list(map(tuple, hourly[['DACCOUNTID', 'DMETERNO']].values)))\n",
    "hourly['ID'] = ids.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find gas records\n",
    "daily = daily.loc[daily['UOM'] == 'CCF']\n",
    "daily = daily.drop(columns=['UOM'])\n",
    "hourly = hourly.loc[hourly['UOM'] == 'CCF']\n",
    "hourly = hourly.drop(columns=['UOM'])\n",
    "\n",
    "# convert to datetime\n",
    "daily['Dt'] =  pd.to_datetime(daily['METERREADDATE'])\n",
    "hourly['Dt'] =  pd.to_datetime(hourly['METERREADDATE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load customer data\n",
    "customer_in = pd.read_excel(location+'PECO Zip Customer 2018.10.01 v2.xlsx', sheet_name=\"Account\")\n",
    "\n",
    "# create id tuple\n",
    "ids = pd.Series(list(map(tuple, customer_in[['DACCOUNTID', 'DMETERNO']].values)))\n",
    "customer_in['ID'] = ids.values\n",
    "customer = customer_in.drop(columns=['CITY', 'STATE', 'ZIPCODE', 'COUNTYCODE'])\n",
    "del customer_in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Restructuring ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions for naming consistency\n",
    "def decrement(x, startswith, split):\n",
    "    \"\"\"\n",
    "    decrements a passed string of form \"demo#\" by 1\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x : string to be decremented\n",
    "    split : string to split on\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    y : decremented string\n",
    "    \"\"\"\n",
    "    if x.startswith(startswith):\n",
    "        a,b = x.split(split)\n",
    "        b = int(b)-1\n",
    "        y = a + split + str(b)\n",
    "\n",
    "        return y\n",
    "\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "    \n",
    "def interval_to_hour(df):\n",
    "    \"\"\"\n",
    "    function for fast rename/relabel during tidying process\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas data frame\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    df : data frame with updated column names\n",
    "    \"\"\"\n",
    "    \n",
    "    df = df.rename(columns=lambda x: decrement(x, \"INTERVAL_\", \"_\"))\n",
    "    df = df.rename(columns=lambda x: x.replace(\"INTERVAL_\", \"HR\"))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename for consistency\n",
    "hourly = interval_to_hour(hourly)\n",
    "hourly = hourly.drop(columns=['METERREADDATE','HR24'])\n",
    "daily = daily.drop(columns=['METERREADDATE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tidy / Stack data (transform into tall data - one row per customer per hour):\n",
    "# ref: http://www.jeannicholashould.com/tidy-data-in-python.html\n",
    "tidy_hourly = pd.melt(hourly, \n",
    "                      id_vars=['ID','DACCOUNTID','DMETERNO','Dt'],\n",
    "                      var_name='Hour', value_name='Use')\n",
    "\n",
    "tidy_hourly_agg = tidy_hourly.groupby(['ID','DACCOUNTID','DMETERNO','Dt']).sum().reset_index()\n",
    "#tidy_hourly_agg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join hourly & daily\n",
    "dfs = [tidy_hourly_agg, daily]\n",
    "use = pd.concat(dfs, keys=['ID','DACCOUNTID', 'DMETERNO', 'Dt', 'Use'], sort=True).reset_index(drop=True)\n",
    "#use.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # merge use data\n",
    "# # hourly_in = [use_oct_in, use_nov_in, use_dec_in, use_jan_in, use_feb_in, use_mar_in, \n",
    "# #              use_apr_in, use_may_in, use_jun_in, use_jul_in, use_aug_in, use_sep_in]\n",
    "# use = pd.concat([use_oct_in, use_nov_in, use_dec_in, use_jan_in, use_feb_in, use_mar_in, \n",
    "#                  use_apr_in, use_may_in, use_jun_in, use_jul_in, use_aug_in, use_sep_in])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "who_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # drop unused tables for memory space\n",
    "# del (use_oct_in, use_nov_in, use_dec_in, use_jan_in, use_feb_in, use_mar_in, \n",
    "#      use_apr_in, use_may_in, use_jun_in, use_jul_in, use_aug_in, use_sep_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save as pickle so we don't have to do this again\n",
    "# use.to_pickle(location+'hourly_use_raw.pkl.zip')\n",
    "# del use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pickle\n",
    "start = timer()\n",
    "\n",
    "use = pd.read_pickle(location+'daily_use_raw.pkl.zip')\n",
    "\n",
    "end = timer()\n",
    "print((end - start)/60) # Time in minutes (.19 - MUCH FASTER!!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append rate code data\n",
    "use = use.merge(customer, how='inner', on=['ID','DACCOUNTID','DMETERNO'])\n",
    "#use.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add dummy variables for day-of-week\n",
    "#use = use.join(pd.get_dummies(use['Dt'].dt.weekday_name))\n",
    "use['Weekday'] = pd.get_dummies(use['Dt'].dt.weekday < 5)[True]\n",
    "\n",
    "# Add dummy variables for holidays\n",
    "from pandas.tseries.holiday import USFederalHolidayCalendar\n",
    "cal = USFederalHolidayCalendar()\n",
    "dr = pd.date_range(start=min(use['Dt']), end=max(use['Dt']))\n",
    "holidays = cal.holidays(start=dr.min(), end=dr.max())\n",
    "\n",
    "use['Holiday'] = pd.get_dummies(use['Dt'].isin(holidays))[True]\n",
    "#use = use.rename(columns={True:'Holiday'})\n",
    "#use = use.drop(columns=[False])\n",
    "#use.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort by date & time\n",
    "use = use.sort_values(by=[\"Dt\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quick look at amount of data we have per ID ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop records with missing data\n",
    "if len(use) == len(use.dropna(subset=['Use'])):\n",
    "    print(f'There is no missing data in the {len(use)} rows of useage data')\n",
    "else:\n",
    "    use = use.dropna(subset=['Use'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"There are {len(use['DACCOUNTID'].drop_duplicates())} unique AccountIDs in the data\")\n",
    "print(f\"There are {len(use['DMETERNO'].drop_duplicates())} unique MeterNos in the data\")\n",
    "ids = use['ID'].drop_duplicates()\n",
    "print(f\"There are {len(ids)} unique AccountID / MeterNo pairs in the data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "ids_to_plot = use.groupby('ID').count()\n",
    "sns.distplot(ids_to_plot['Use'], bins=20, kde=False, rug=False)\n",
    "plt.title('Frequency of IDs with X dates')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(ids_to_plot[ids_to_plot['Use'] > 330]['Use'], bins=20, kde=False, rug=False)\n",
    "plt.title('Frequency of IDs with at least 330 dates')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i in range(1,365):\n",
    "#    print(f\"{ids_to_plot[ids_to_plot['Use'] >= i]['Use'].count()} records have >={i} dates\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find IDs with sufficient data to build initial segments on --> reliable training set ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find IDs with at least 360 days of data\n",
    "sufficient_ids = ids_to_plot[ids_to_plot['Use'] >= 360].reset_index()['ID']\n",
    "print(f\"{100*len(sufficient_ids)/len(use['ID'].unique())} percent of IDs have at least 360 days of data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the dates for which sufficient_ids all have data\n",
    "pivoted = (use[use['ID'].isin(sufficient_ids)]\n",
    "           .pivot_table(index='Dt', columns=['ID'], values='Use', fill_value=None))\n",
    "sufficient_dates = pivoted.dropna().reset_index()['Dt']\n",
    "print(f\"The IDs with sufficient data overlap on {len(sufficient_dates)} days of the most recent year of data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter total use dataset to contain only IDs who have sufficient data and only dates where sufficient_ids have data\n",
    "sufficient_use = use[(use['ID'].isin(sufficient_ids)) & (use['Dt'].isin(sufficient_dates))]\n",
    "#sufficient_use.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sanity check\n",
    "print(len(use))\n",
    "print(len(sufficient_use))\n",
    "print(len(ids))\n",
    "print(len(sufficient_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subgroup detection ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(use[['ID','REVENUCODE']].drop_duplicates().groupby('REVENUCODE').count())\n",
    "print(\"nan           \"+str(len(use[np.isnan(use['REVENUCODE'])][['ID']].drop_duplicates())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(use[['ID','REVENUCODE']].drop_duplicates().groupby('REVENUCODE').count())\n",
    "print(\"nan           \"+str(len(use[np.isnan(use['REVENUCODE'])][['ID']].drop_duplicates())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recode 'REVENUCODE' as 'TYPE'\n",
    "def recode(df):\n",
    "    \"\"\"combines REVENUCODE groups 1,12,nan to HOME and 3,5 to COMMERICAL\"\"\"\n",
    "    df['TYPE'] = None\n",
    "    df.loc[ df.REVENUCODE == 1, 'TYPE' ] = 'HOME'\n",
    "    df.loc[ df.REVENUCODE == 12, 'TYPE' ] = 'HOME'\n",
    "    df.loc[ np.isnan(df['REVENUCODE']), 'TYPE' ] = 'HOME'\n",
    "    df.loc[ df.REVENUCODE == 3, 'TYPE' ] = 'COMM'\n",
    "    df.loc[ df.REVENUCODE == 5, 'TYPE' ] = 'COMM'\n",
    "    \n",
    "    #df['TYPE'][df['REVENUCODE']==1] = 'HOME'\n",
    "    #df['TYPE'][df['REVENUCODE']==12] = 'HOME'\n",
    "    #df['TYPE'][np.isnan(df['REVENUCODE'])] = 'HOME'\n",
    "    #df['TYPE'][df['REVENUCODE']==3] = 'COMM'\n",
    "    #df['TYPE'][df['REVENUCODE']==5] = 'COMM'\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore errors\n",
    "use = recode(use)\n",
    "sufficient_use = recode(sufficient_use)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save file(s) as pickles\n",
    "# using save location to save with other data files outside of git repo\n",
    "# data.to_csv(location+'peco.csv', sep='\\t')\n",
    "use.to_pickle(location+'peco_daily.pkl.zip')\n",
    "sufficient_use.to_pickle(location+'peco_sufficient_daily.pkl.zip')\n",
    "ids.to_pickle(location+'peco_daily_ids.pkl.zip')\n",
    "sufficient_ids.to_pickle(location+'peco_sufficient_daily_ids.pkl.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Merge use & weather? ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "daily = pd.read_pickle(location+'peco_daily.pkl.zip')\n",
    "sufficient_daily = pd.read_pickle(location+'peco_sufficient_daily.pkl.zip')\n",
    "weather = pd.read_pickle(location+'daily_weather.pkl.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge\n",
    "daily = pd.merge(daily, weather, how='inner', on=['Dt'])\n",
    "sufficient_daily = pd.merge(sufficient_daily, weather, how='inner', on=['Dt'])\n",
    "# CustIDs || Date | Consumption |||| Weather_variables"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

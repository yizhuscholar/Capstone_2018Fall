{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hourly Data #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning & Checking "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* import\n",
    "* sample size\n",
    "* % missing\n",
    "* outliers\n",
    "* distributions(?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data\n",
    "location = '/Users/mithras/Documents/_SCHOOL/_Drexel/BUSN 710 - Capstone/Data/Forecasting Project/'\n",
    "\n",
    "# hourly data\n",
    "use_jan_in = pd.read_excel(location+'Zip_HourlylUsage_2018.01.xlsx')\n",
    "use_jul_in = pd.read_excel(location+'Zip_HourlylUsage_2018.07.xlsx')\n",
    "\n",
    "# daily data\n",
    "use_oct_in = pd.read_excel(location+'Sample Usage_2017.10 Oct.xlsx')\n",
    "use_nov_in = pd.read_excel(location+'Sample Usage_2017.11 Nov.xlsx')\n",
    "use_dec_in = pd.read_excel(location+'Sample Usage_2017.12 Dec.xlsx')\n",
    "# use_jan_in = pd.read_excel(location+'Sample Usage_2018.01 Jan.xlsx')\n",
    "use_feb_in = pd.read_excel(location+'Sample Usage_2018.02 Feb.xlsx')\n",
    "use_mar_in = pd.read_excel(location+'Sample Usage_2018.03 March.xlsx')\n",
    "use_apr_in = pd.read_excel(location+'Sample Usage_2018.04 April.xlsx')\n",
    "use_may_in = pd.read_excel(location+'Sample Usage_2018.05 May.xlsx')\n",
    "use_jun_in = pd.read_excel(location+'Sample Usage_2018.06 June.xlsx')\n",
    "# use_jul_in = pd.read_excel(location+'Sample Usage_2018.07 Jul.xlsx')\n",
    "use_aug_in = pd.read_excel(location+'Sample Usage_2018.08 Aug.xlsx')\n",
    "use_sep_in = pd.read_excel(location+'Sample Usage_2018.09 Sep.xlsx')\n",
    "\n",
    "# weather data\n",
    "temp_in = pd.read_excel(location+'Weather Data for Drexel 9_28_2018.xlsx', sheet_name=\"TMP\")\n",
    "humid_in = pd.read_excel(location+'Weather Data for Drexel 9_28_2018.xlsx', sheet_name=\"HUM\")\n",
    "wind_in = pd.read_excel(location+'Weather Data for Drexel 9_28_2018.xlsx', sheet_name=\"WSP\")\n",
    "cloud_in = pd.read_excel(location+'Weather Data for Drexel 9_28_2018.xlsx', sheet_name=\"CC\")\n",
    "\n",
    "# other data\n",
    "customer_in = pd.read_excel(location+'Plain Customer Drexel 2018.09.27.xlsx')\n",
    "ratecode_in = pd.read_excel(location+'Rate Codes for Drexel 9_28_2018.xlsx')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge use data\n",
    "daily_in = [use_oct_in, use_nov_in, use_dec_in, use_feb_in, use_mar_in, \n",
    "            use_apr_in, use_may_in, use_jun_in, use_aug_in, use_sep_in]\n",
    "daily = pd.concat(daily_in)\n",
    "daily = daily.rename(columns={'DAccountID':'DACCOUNTID', 'DMeterNo':'DMETERNO',\n",
    "                              'DAILY_INTERVAL_USAGE':'Use'})\n",
    "\n",
    "hourly_in = [use_jan_in, use_jul_in]\n",
    "hourly = pd.concat(hourly_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify unique ids with full history\n",
    "def uniqueIDs(dfs, cols):\n",
    "    \"\"\"\n",
    "    Identifies uniques in specified cols and intersects across dfs to return only the overlap\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dfs : list of pandas data frames with consistent headers\n",
    "    cols : list of column names to track\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    ids : pandas df with cols columns containing only ids present in all dfs\n",
    "    \"\"\"\n",
    "    \n",
    "    dfs2 = []\n",
    "    for df in dfs:\n",
    "        dfs2.append(df.loc[:, df.columns.isin(cols)].drop_duplicates())\n",
    "   \n",
    "    ids = dfs2[0]\n",
    "    for i in range(1,len(dfs2)):\n",
    "        ids = pd.merge(ids, dfs2[i], how='inner')\n",
    "        \n",
    "    return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get list of unique ids present across all months\n",
    "daily_ids = uniqueIDs(daily_in,['DAccountID','DMeterNo'])\n",
    "daily_ids = daily_ids.rename(columns={'DAccountID':'DACCOUNTID', 'DMeterNo':'DMETERNO'})\n",
    "\n",
    "hourly_ids = uniqueIDs(hourly_in,['DACCOUNTID','DMETERNO'])\n",
    "\n",
    "ids = uniqueIDs([daily_ids,hourly_ids],['DACCOUNTID','DMETERNO'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean\n",
    "daily = daily.loc[daily['UOM'] == 'CCF']\n",
    "daily = daily.drop(columns=['UOM'])\n",
    "hourly = hourly.loc[hourly['UOM'] == 'CCF']\n",
    "hourly = hourly.drop(columns=['UOM'])\n",
    "\n",
    "temp = temp_in.drop(columns=['HR0','HR1','HR2','HR3','HR4','HR5','HR6','HR7','HR8','HR9','HR10','HR11','HR12',\n",
    "                             'HR13','HR14','HR15','HR16','HR17','HR18','HR19','HR20','HR21','HR22','HR23',\n",
    "                             'AvgHL','Gas Day Average','HDD-HL','CDD-HL','HDD-24','CDD-24',\n",
    "                             'Unnamed: 34','Unnamed: 35'])\n",
    "temp = temp.rename(columns={'Avg':'AvgTemp'})\n",
    "\n",
    "humid = humid_in.drop(columns=['HR0','HR1','HR2','HR3','HR4','HR5','HR6','HR7','HR8','HR9','HR10','HR11','HR12',\n",
    "                               'HR13','HR14','HR15','HR16','HR17','HR18','HR19','HR20','HR21','HR22','HR23',\n",
    "                               'Unnamed: 26','Unnamed: 27','Unnamed: 28'])\n",
    "humid = humid.rename(columns={'Avg':'AvgHumid'})\n",
    "\n",
    "wind = wind_in.drop(columns=['HR0','HR1','HR2','HR3','HR4','HR5','HR6','HR7','HR8','HR9','HR10','HR11','HR12',\n",
    "                             'HR13','HR14','HR15','HR16','HR17','HR18','HR19','HR20','HR21','HR22','HR23',\n",
    "                             'Unnamed: 26'])\n",
    "wind = wind.rename(columns={'Avg':'AvgWind'})\n",
    "\n",
    "cloud = cloud_in.drop(columns=['HR0','HR1','HR2','HR3','HR4','HR5','HR6','HR7','HR8','HR9','HR10','HR11','HR12',\n",
    "                               'HR13','HR14','HR15','HR16','HR17','HR18','HR19','HR20','HR21','HR22','HR23'])\n",
    "cloud = cloud.rename(columns={'AvgDaytime':'AvgDaytimeCloud', 'Avg':'AvgCloud'})\n",
    "\n",
    "# convert to datetime\n",
    "daily['Dt'] =  pd.to_datetime(daily['METERREADDATE'])\n",
    "hourly['Dt'] =  pd.to_datetime(hourly['METERREADDATE'])\n",
    "temp['Dt'] =  pd.to_datetime(temp['Dt'])\n",
    "humid['Dt'] =  pd.to_datetime(humid['Dt'])\n",
    "wind['Dt'] =  pd.to_datetime(wind['Dt'])\n",
    "cloud['Dt'] =  pd.to_datetime(cloud['Dt'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Restructuring ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions for naming consistency\n",
    "def decrement(x, startswith, split):\n",
    "    \"\"\"\n",
    "    decrements a passed string of form \"demo#\" by 1\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x : string to be decremented\n",
    "    split : string to split on\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    y : decremented string\n",
    "    \"\"\"\n",
    "    if x.startswith(startswith):\n",
    "        a,b = x.split(split)\n",
    "        b = int(b)-1\n",
    "        y = a + split + str(b)\n",
    "\n",
    "        return y\n",
    "\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "    \n",
    "def interval_to_hour(df):\n",
    "    \"\"\"\n",
    "    function for fast rename/relabel during tidying process\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas data frame\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    df : data frame with updated column names\n",
    "    \"\"\"\n",
    "    \n",
    "    df = df.rename(columns=lambda x: decrement(x, \"INTERVAL_\", \"_\"))\n",
    "    df = df.rename(columns=lambda x: x.replace(\"INTERVAL_\", \"HR\"))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename for consistency\n",
    "hourly = interval_to_hour(hourly)\n",
    "hourly = hourly.drop(columns=['METERREADDATE','HR24'])\n",
    "daily = daily.drop(columns=['METERREADDATE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tidy / Stack data (transform into tall data - one row per customer per hour):\n",
    "# ref: http://www.jeannicholashould.com/tidy-data-in-python.html\n",
    "tidy_hourly = pd.melt(hourly, \n",
    "                      id_vars=['DACCOUNTID','DMETERNO','Dt'],\n",
    "                      var_name='Hour', value_name='Use')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tidy_hourly_agg = tidy_hourly.groupby(['DACCOUNTID','DMETERNO','Dt']).sum().reset_index()\n",
    "# tidy_hourly_agg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join hourly & daily\n",
    "dfs = [tidy_hourly_agg, daily]\n",
    "use = pd.concat(dfs, keys=['DACCOUNTID', 'DMETERNO', 'Dt', 'Use'], sort=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort by date & time\n",
    "use = use.sort_values(by=[\"Dt\"])\n",
    "daily = daily.sort_values(by=[\"Dt\"])\n",
    "temp = temp.sort_values(by=[\"Dt\"])\n",
    "humid = humid.sort_values(by=[\"Dt\"])\n",
    "wind = wind.sort_values(by=[\"Dt\"])\n",
    "cloud = cloud.sort_values(by=[\"Dt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### LAGS/DELTAS ARE HERE IN HOURLY IPYNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge\n",
    "weather1 = pd.merge(temp, humid, how='inner', on=['Dt'])\n",
    "weather2 = pd.merge(wind, cloud, how='inner', on=['Dt'])\n",
    "weather = pd.merge(weather1, weather2, how='inner', on=['Dt'])\n",
    "\n",
    "daily = pd.merge(use, weather, how='inner', on=['Dt'])\n",
    "# CustIDs | Date | Time | Consumption | Weather_variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add dummy variables for day-of-week and holidays\n",
    "daily = daily.join(pd.get_dummies(daily['Dt'].dt.weekday_name))\n",
    "\n",
    "from pandas.tseries.holiday import USFederalHolidayCalendar\n",
    "cal = USFederalHolidayCalendar()\n",
    "dr = pd.date_range(start='2010-01-01', end='2018-12-31')\n",
    "holidays = cal.holidays(start=dr.min(), end=dr.max())\n",
    "\n",
    "daily = daily.join(pd.get_dummies(daily['Dt'].isin(holidays)))\n",
    "daily = daily.rename(columns={True:'Holiday'})\n",
    "daily = daily.drop(columns=[False])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only records with full year of data\n",
    "daily = daily.merge(ids, how='inner', on=['DACCOUNTID','DMETERNO'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save file(s) as pickles\n",
    "# using save location to save with other data files outside of git repo\n",
    "# data.to_csv(location+'peco.csv', sep='\\t')\n",
    "daily.to_pickle(location+'peco_daily.pkl.zip') "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forecasting Data #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define data location\n",
    "#location = '/Users/mithras/Documents/_SCHOOL/_Drexel/BUSN 710 - Capstone/Data/Forecasting Project/'\n",
    "location = '/Users/loki/Documents/Data/Forecasting Project/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning & Checking "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hourly data\n",
    "use_oct_in = pd.read_excel(location+'PECO Zip HourlyUsage_2017.10.xlsx')\n",
    "use_nov_in = pd.read_excel(location+'PECO Zip HourlyUsage_2017.11.xlsx') \n",
    "use_dec_in = pd.read_excel(location+'PECO Zip HourlyUsage_2017.12.xlsx') \n",
    "use_jan_in = pd.read_excel(location+'Zip_HourlylUsage_2018.01.xlsx')\n",
    "use_feb_in = pd.read_excel(location+'PECO Zip HourlyUsage_2018.02.xlsx')\n",
    "use_mar_in = pd.read_excel(location+'PECO Zip HourlyUsage_2018.03.xlsx')\n",
    "use_apr_in = pd.read_excel(location+'PECO Zip HourlyUsage_2018.04.xlsx')\n",
    "use_may_in = pd.read_excel(location+'PECO Zip HourlyUsage_2018.05.xlsx')\n",
    "use_jun_in = pd.read_excel(location+'PECO Zip HourlyUsage_2018.06.xlsx')\n",
    "use_jul_in = pd.read_excel(location+'Zip_HourlylUsage_2018.07.xlsx')\n",
    "use_aug_in = pd.read_excel(location+'PECO Zip HourlyUsage_2018.08.xlsx')\n",
    "use_sep_in = pd.read_excel(location+'PECO Zip HourlyUsage_2018.09.xlsx')\n",
    "\n",
    "use_oct_in2 = pd.read_excel(location+'PECO Zip HourlyUsage_2016.10.xlsx')\n",
    "use_nov_in2 = pd.read_excel(location+'PECO Zip HourlyUsage_2016.11.xlsx') \n",
    "use_dec_in2 = pd.read_excel(location+'PECO Zip HourlyUsage_2016.12.xlsx') \n",
    "use_jan_in2 = pd.read_excel(location+'PECO Zip HourlyUsage_2017.01.xlsx')\n",
    "use_feb_in2 = pd.read_excel(location+'PECO Zip HourlyUsage_2017.02.xlsx')\n",
    "use_mar_in2 = pd.read_excel(location+'PECO Zip HourlyUsage_2017.03.xlsx')\n",
    "use_apr_in2 = pd.read_excel(location+'PECO Zip HourlyUsage_2017.04.xlsx')\n",
    "use_may_in2 = pd.read_excel(location+'PECO Zip HourlyUsage_2017.05.xlsx')\n",
    "use_jun_in2 = pd.read_excel(location+'PECO Zip HourlyUsage_2017.06.xlsx')\n",
    "use_jul_in2 = pd.read_excel(location+'PECO Zip HourlyUsage_2017.07.xlsx')\n",
    "use_aug_in2 = pd.read_excel(location+'PECO Zip HourlyUsage_2017.08.xlsx')\n",
    "use_sep_in2 = pd.read_excel(location+'PECO Zip HourlyUsage_2017.09.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge use data\n",
    "hourly_in = [use_oct_in, use_nov_in, use_dec_in, use_jan_in, use_feb_in, use_mar_in, \n",
    "             use_apr_in, use_may_in, use_jun_in, use_jul_in, use_aug_in, use_sep_in,\n",
    "             use_oct_in2, use_nov_in2, use_dec_in2, use_jan_in2, use_feb_in2, use_mar_in2, \n",
    "             use_apr_in2, use_may_in2, use_jun_in2, use_jul_in2, use_aug_in2, use_sep_in2]\n",
    "use = pd.concat(hourly_in)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create ID tuple\n",
    "ids = pd.Series(list(map(tuple, use[['DACCOUNTID', 'DMETERNO']].values)))\n",
    "use['ID'] = ids.values\n",
    "\n",
    "ids = pd.Series(list(map(tuple, customer_in[['DACCOUNTID', 'DMETERNO']].values)))\n",
    "customer_in['ID'] = ids.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find gas records\n",
    "use = use.loc[use['UOM'] == 'CCF']\n",
    "use = use.drop(columns=['UOM'])\n",
    "\n",
    "customer = customer_in.drop(columns=['CITY', 'STATE', 'ZIPCODE', 'COUNTYCODE'])\n",
    "\n",
    "# convert to datetime\n",
    "use['Dt'] =  pd.to_datetime(use['METERREADDATE'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Restructuring ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions for naming consistency\n",
    "def decrement(x, startswith, split):\n",
    "    \"\"\"\n",
    "    decrements a passed string of form \"demo#\" by 1\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x : string to be decremented\n",
    "    split : string to split on\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    y : decremented string\n",
    "    \"\"\"\n",
    "    if x.startswith(startswith):\n",
    "        a,b = x.split(split)\n",
    "        b = int(b)-1\n",
    "        y = a + split + str(b)\n",
    "\n",
    "        return y\n",
    "\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "    \n",
    "def interval_to_hour(df):\n",
    "    \"\"\"\n",
    "    function for fast rename/relabel during tidying process\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas data frame\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    df : data frame with updated column names\n",
    "    \"\"\"\n",
    "    \n",
    "    df = df.rename(columns=lambda x: decrement(x, \"INTERVAL_\", \"_\"))\n",
    "    df = df.rename(columns=lambda x: x.replace(\"INTERVAL_\", \"HR\"))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename for consistency\n",
    "use = interval_to_hour(use)\n",
    "use = use.drop(columns=['METERREADDATE','HR24'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tidy / Stack data (transform into tall data - one row per customer per hour):\n",
    "# ref: http://www.jeannicholashould.com/tidy-data-in-python.html\n",
    "tidy_use = pd.melt(use, \n",
    "                   id_vars=['ID','DACCOUNTID','DMETERNO','Dt'],\n",
    "                   var_name='Hour', value_name='Use')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# relabel/retype\n",
    "tidy_use['Hour'] = tidy_use['Hour'].str.extract('(\\d+)').astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append rate code data\n",
    "tidy_use = tidy_use.merge(customer, how='inner', on=['ID','DACCOUNTID','DMETERNO'])\n",
    "#tidy_use.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make daily data ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for daily aggregation\n",
    "daily_use = tidy_use.groupby(['ID','DACCOUNTID','DMETERNO','Dt']).sum().reset_index()\n",
    "\n",
    "# Add dummy variables for day-of-week\n",
    "daily_use['Weekday'] = pd.get_dummies(daily_use['Dt'].dt.weekday < 5)[True]\n",
    "\n",
    "# Add dummy variables for holidays\n",
    "from pandas.tseries.holiday import USFederalHolidayCalendar\n",
    "cal = USFederalHolidayCalendar()\n",
    "dr = pd.date_range(start=min(daily_use['Dt']), end=max(daily_use['Dt']))\n",
    "holidays = cal.holidays(start=dr.min(), end=dr.max())\n",
    "\n",
    "daily_use['Holiday'] = pd.get_dummies(daily_use['Dt'].isin(holidays))[True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find IDs with at least 720 days of data\n",
    "ids_to_plot = daily_use.groupby('ID').count()\n",
    "sufficient_ids = ids_to_plot[ids_to_plot['Use'] >= 720].reset_index()['ID']\n",
    "print(f\"{100*len(sufficient_ids)/len(use['ID'].unique())} percent of IDs have at least 720 days of data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the dates for which sufficient_ids all have data\n",
    "pivoted = (daily_use[daily_use['ID'].isin(sufficient_ids)]\n",
    "           .pivot_table(index='Dt', columns=['ID'], values='Use', fill_value=None))\n",
    "sufficient_dates = pivoted.dropna().reset_index()['Dt']\n",
    "print(f\"The IDs with sufficient data overlap on {len(sufficient_dates)} days of the most recent year of data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter total use dataset to contain only IDs who have sufficient data and only dates where sufficient_ids have data\n",
    "sufficient_use = daily_use[(daily_use['ID'].isin(sufficient_ids)) & (daily_use['Dt'].isin(sufficient_dates))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save file(s) as pickles\n",
    "# using save location to save with other data files outside of git repo\n",
    "# data.to_csv(location+'peco.csv', sep='\\t')\n",
    "daily_use.to_pickle(location+'fcast_daily.pkl.zip')\n",
    "sufficient_use.to_pickle(location+'fcast_sufficient_daily.pkl.zip')\n",
    "ids.to_pickle(location+'fcast_daily_ids.pkl.zip')\n",
    "sufficient_ids.to_pickle(location+'fcast_sufficient_daily_ids.pkl.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make hourly data###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create single column for date and time\n",
    "tidy_use['year'] = pd.to_datetime(tidy_use['Dt'].values).year\n",
    "tidy_use['month'] = pd.to_datetime(tidy_use['Dt'].values).month\n",
    "tidy_use['day'] = pd.to_datetime(tidy_use['Dt'].values).day\n",
    "tidy_use['datetime'] = pd.to_datetime(tidy_use[['year','month','day','Hour']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add dummy variables for day-of-week\n",
    "#use = use.join(pd.get_dummies(use['Dt'].dt.weekday_name))\n",
    "tidy_use['Weekday'] = pd.get_dummies(tidy_use['Dt'].dt.weekday < 5)[True]\n",
    "\n",
    "# Add dummy variables for hour-of-day\n",
    "tidy_use = tidy_use.join(pd.get_dummies(tidy_use['Hour'],prefix='HR'))\n",
    "\n",
    "# Add dummy variables for part-of-day\n",
    "tidy_use['AM'] = tidy_use[(tidy_use[Dt].dt.hour >= 5) & (tidy_use[Dt].dt.hour < 9)] # 6-9 am\n",
    "tidy_use['DAY'] = tidy_use[(tidy_use[Dt].dt.hour >= 9) & (tidy_use[Dt].dt.hour < 16)] # 9 am-5 pm\n",
    "tidy_use['EVE'] = tidy_use[(tidy_use[Dt].dt.hour >= 16) & (tidy_use[Dt].dt.hour < 21)] # 5-10 pm\n",
    "tidy_use['NIGHT'] = tidy_use[(tidy_use[Dt].dt.hour >= 21) & (tidy_use[Dt].dt.hour < 5)] # 11 pm-6 am\n",
    "\n",
    "# Add dummy variables for holidays\n",
    "from pandas.tseries.holiday import USFederalHolidayCalendar\n",
    "cal = USFederalHolidayCalendar()\n",
    "dr = pd.date_range(start=min(tidy_use['Dt']), end=max(tidy_use['Dt']))\n",
    "holidays = cal.holidays(start=dr.min(), end=dr.max())\n",
    "\n",
    "tidy_use['Holiday'] = pd.get_dummies(tidy_use['Dt'].isin(holidays))[True]\n",
    "#tidy_use = tidy_use.rename(columns={True:'Holiday'})\n",
    "#tidy_use = tidy_use.drop(columns=[False])\n",
    "#tidy_use.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort by date & time\n",
    "tidy_use = tidy_use.drop(columns=['Dt','year','month','day','Hour'])\n",
    "tidy_use = tidy_use.sort_values(by=['datetime'])\n",
    "tidy_use = tidy_use.rename(columns={'datetime':'Dt'}) # for consistency\n",
    "#tidy_use.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find IDs with at least 720 days of data\n",
    "ids_to_plot = tidy_use.groupby('ID').count()\n",
    "sufficient_ids = ids_to_plot[ids_to_plot['Use'] >= 720].reset_index()['ID']\n",
    "print(f\"{100*len(sufficient_ids)/len(use['ID'].unique())} percent of IDs have at least 720 days of data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the dates for which sufficient_ids all have data\n",
    "pivoted = (tidy_use[tidy_use['ID'].isin(sufficient_ids)]\n",
    "           .pivot_table(index='Dt', columns=['ID'], values='Use', fill_value=None))\n",
    "sufficient_dates = pivoted.dropna().reset_index()['Dt']\n",
    "print(f\"The IDs with sufficient data overlap on {len(sufficient_dates)} days of the most recent year of data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter total use dataset to contain only IDs who have sufficient data and only dates where sufficient_ids have data\n",
    "sufficient_use = tidy_use[(tidy_use['ID'].isin(sufficient_ids)) & (tidy_use['Dt'].isin(sufficient_dates))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save file(s) as pickles\n",
    "# using save location to save with other data files outside of git repo\n",
    "# data.to_csv(location+'peco.csv', sep='\\t')\n",
    "tidy_use.to_pickle(location+'fcast_hourly.pkl.zip')\n",
    "sufficient_use.to_pickle(location+'fcast_sufficient_hourly.pkl.zip')\n",
    "ids.to_pickle(location+'fcast_daily_ids.pkl.zip')\n",
    "sufficient_ids.to_pickle(location+'fcast_sufficient_hourly_ids.pkl.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Merge use & weather? ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "hourly = pd.read_pickle(location+'peco_hourly.pkl.zip')\n",
    "sufficient_hourly = pd.read_pickle(location+'peco_sufficient_hourly.pkl.zip')\n",
    "weather = pd.read_pickle(location+'hourly_weather.pkl.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge\n",
    "hourly = pd.merge(hourly, weather, how='inner', on=['Dt'])\n",
    "sufficient_hourly = pd.merge(sufficient_hourly, weather, how='inner', on=['Dt'])\n",
    "# CustIDs || Date | Consumption |||| Weather_variables "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

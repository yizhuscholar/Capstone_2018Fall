{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forecasting Data #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from timeit import default_timer as timer\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define data location\n",
    "#location = '/Users/mithras/Documents/_SCHOOL/_Drexel/BUSN 710 - Capstone/Data/Forecasting Project/'\n",
    "location = '/Users/loki/Documents/Data/Forecasting Project/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning & Checking "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25.6850781347\n"
     ]
    }
   ],
   "source": [
    "# # hourly data\n",
    "# start = timer()\n",
    "\n",
    "# use_oct_in = pd.read_excel(location+'PECO Zip HourlyUsage_2017.10.xlsx')\n",
    "# use_nov_in = pd.read_excel(location+'PECO Zip HourlyUsage_2017.11.xlsx') \n",
    "# use_dec_in = pd.read_excel(location+'PECO Zip HourlyUsage_2017.12.xlsx') \n",
    "# use_jan_in = pd.read_excel(location+'Zip_HourlylUsage_2018.01.xlsx')\n",
    "# use_feb_in = pd.read_excel(location+'PECO Zip HourlyUsage_2018.02.xlsx')\n",
    "# use_mar_in = pd.read_excel(location+'PECO Zip HourlyUsage_2018.03.xlsx')\n",
    "# use_apr_in = pd.read_excel(location+'PECO Zip HourlyUsage_2018.04.xlsx')\n",
    "# use_may_in = pd.read_excel(location+'PECO Zip HourlyUsage_2018.05.xlsx')\n",
    "# use_jun_in = pd.read_excel(location+'PECO Zip HourlyUsage_2018.06.xlsx')\n",
    "# use_jul_in = pd.read_excel(location+'Zip_HourlylUsage_2018.07.xlsx')\n",
    "# use_aug_in = pd.read_excel(location+'PECO Zip HourlyUsage_2018.08.xlsx')\n",
    "# use_sep_in = pd.read_excel(location+'PECO Zip HourlyUsage_2018.09.xlsx')\n",
    "\n",
    "# use_oct_in2 = pd.read_excel(location+'PECO Zip HourlyUsage_2016.10.xlsx')\n",
    "# use_nov_in2 = pd.read_excel(location+'PECO Zip HourlyUsage_2016.11.xlsx') \n",
    "# use_dec_in2 = pd.read_excel(location+'PECO Zip HourlyUsage_2016.12.xlsx') \n",
    "# use_jan_in2 = pd.read_excel(location+'PECO Zip HourlyUsage_2017.01.xlsx')\n",
    "# use_feb_in2 = pd.read_excel(location+'PECO Zip HourlyUsage_2017.02.xlsx')\n",
    "# use_mar_in2 = pd.read_excel(location+'PECO Zip HourlyUsage_2017.03.xlsx')\n",
    "# use_apr_in2 = pd.read_excel(location+'PECO Zip HourlyUsage_2017.04.xlsx')\n",
    "# use_may_in2 = pd.read_excel(location+'PECO Zip HourlyUsage_2017.05.xlsx')\n",
    "# use_jun_in2 = pd.read_excel(location+'PECO Zip HourlyUsage_2017.06.xlsx')\n",
    "# use_jul_in2 = pd.read_excel(location+'PECO Zip HourlyUsage_2017.07.xlsx')\n",
    "# use_aug_in2 = pd.read_excel(location+'PECO Zip HourlyUsage_2017.08.xlsx')\n",
    "# use_sep_in2 = pd.read_excel(location+'PECO Zip HourlyUsage_2017.09.xlsx')\n",
    "\n",
    "# end = timer()\n",
    "# print((end - start)/60) # Time in minutes (~25 min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # merge use data\n",
    "# hourly_in = [use_oct_in, use_nov_in, use_dec_in, use_jan_in, use_feb_in, use_mar_in, \n",
    "#              use_apr_in, use_may_in, use_jun_in, use_jul_in, use_aug_in, use_sep_in,\n",
    "#              use_oct_in2, use_nov_in2, use_dec_in2, use_jan_in2, use_feb_in2, use_mar_in2, \n",
    "#              use_apr_in2, use_may_in2, use_jun_in2, use_jul_in2, use_aug_in2, use_sep_in2]\n",
    "             \n",
    "# use = pd.concat(hourly_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del (use_oct_in, use_nov_in, use_dec_in, use_jan_in, use_feb_in, use_mar_in, \n",
    "#      use_apr_in, use_may_in, use_jun_in, use_jul_in, use_aug_in, use_sep_in,\n",
    "#      use_oct_in2, use_nov_in2, use_dec_in2, use_jan_in2, use_feb_in2, use_mar_in2, \n",
    "#      use_apr_in2, use_may_in2, use_jun_in2, use_jul_in2, use_aug_in2, use_sep_in2)\n",
    "# del hourly_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save as pickle so we don't have to do this again\n",
    "# use.to_pickle(location+'fcast_use_raw.pkl.zip')\n",
    "# del use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5305368478000067\n"
     ]
    }
   ],
   "source": [
    "# load pickle\n",
    "start = timer()\n",
    "\n",
    "use = pd.read_pickle(location+'fcast_use_raw.pkl.zip')\n",
    "\n",
    "end = timer()\n",
    "print((end - start)/60) # Time in minutes (~.5 min - MUCH FASTER!!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create ID tuple\n",
    "ids = pd.Series(list(map(tuple, use[['DACCOUNTID', 'DMETERNO']].values)))\n",
    "use['ID'] = ids.values\n",
    "\n",
    "# find gas records\n",
    "use = use.loc[use['UOM'] == 'CCF']\n",
    "use = use.drop(columns=['UOM'])\n",
    "\n",
    "# convert to datetime\n",
    "use['Dt'] =  pd.to_datetime(use['METERREADDATE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load customer data\n",
    "customer_in = pd.read_excel(location+'PECO Zip Customer 2018.10.01 v2.xlsx', sheet_name=\"Account\")\n",
    "ids = pd.Series(list(map(tuple, customer_in[['DACCOUNTID', 'DMETERNO']].values)))\n",
    "customer_in['ID'] = ids.values\n",
    "customer = customer_in.drop(columns=['CITY', 'STATE', 'ZIPCODE', 'COUNTYCODE'])\n",
    "del customer_in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Restructuring ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions for naming consistency\n",
    "def decrement(x, startswith, split):\n",
    "    \"\"\"\n",
    "    decrements a passed string of form \"demo#\" by 1\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x : string to be decremented\n",
    "    split : string to split on\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    y : decremented string\n",
    "    \"\"\"\n",
    "    if x.startswith(startswith):\n",
    "        a,b = x.split(split)\n",
    "        b = int(b)-1\n",
    "        y = a + split + str(b)\n",
    "\n",
    "        return y\n",
    "\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "    \n",
    "def interval_to_hour(df):\n",
    "    \"\"\"\n",
    "    function for fast rename/relabel during tidying process\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas data frame\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    df : data frame with updated column names\n",
    "    \"\"\"\n",
    "    \n",
    "    df = df.rename(columns=lambda x: decrement(x, \"INTERVAL_\", \"_\"))\n",
    "    df = df.rename(columns=lambda x: x.replace(\"INTERVAL_\", \"HR\"))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename for consistency\n",
    "use = interval_to_hour(use)\n",
    "use = use.drop(columns=['METERREADDATE','HR24'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tidy / Stack data (transform into tall data - one row per customer per hour):\n",
    "# ref: http://www.jeannicholashould.com/tidy-data-in-python.html\n",
    "tidy_use = pd.melt(use, \n",
    "                   id_vars=['ID','DACCOUNTID','DMETERNO','Dt'],\n",
    "                   var_name='Hour', value_name='Use')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop use from memory\n",
    "del use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# relabel/retype\n",
    "tidy_use['Hour'] = tidy_use['Hour'].str.extract('(\\d+)').astype(int)\n",
    "tidy_use = tidy_use.sort_values(by=[\"Dt\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make daily data ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for daily aggregation\n",
    "daily_use = (tidy_use[['ID','DACCOUNTID','DMETERNO','Use','Dt']]\n",
    "             .groupby(['ID','DACCOUNTID','DMETERNO','Dt'])\n",
    "             .sum()\n",
    "             .reset_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append rate code data\n",
    "daily_use = daily_use.merge(customer, how='inner', on=['ID','DACCOUNTID','DMETERNO'])\n",
    "#daily_use.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add dummy variables for day-of-week\n",
    "daily_use['Weekday'] = pd.get_dummies(daily_use['Dt'].dt.weekday < 5)[True]\n",
    "\n",
    "# Add dummy variables for holidays\n",
    "from pandas.tseries.holiday import USFederalHolidayCalendar\n",
    "cal = USFederalHolidayCalendar()\n",
    "#dr = pd.date_range(start=min(tidy_use['Dt']), end=max(tidy_use['Dt']))\n",
    "dr = pd.date_range(start=\"1-1-2010\", end=\"12-31-2020\")\n",
    "holidays = cal.holidays(start=dr.min(), end=dr.max())\n",
    "\n",
    "daily_use['Holiday'] = pd.get_dummies(daily_use['Dt'].isin(holidays))[True]\n",
    "del (USFederalHolidayCalendar, cal, dr, holidays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>DACCOUNTID</th>\n",
       "      <th>DMETERNO</th>\n",
       "      <th>Dt</th>\n",
       "      <th>Use</th>\n",
       "      <th>DCUSTOMERID</th>\n",
       "      <th>TARIFF</th>\n",
       "      <th>FUELTYPE</th>\n",
       "      <th>REVENUCODE</th>\n",
       "      <th>Weekday</th>\n",
       "      <th>Holiday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(156414999216, 4464803701784)</td>\n",
       "      <td>156414999216</td>\n",
       "      <td>4464803701784</td>\n",
       "      <td>2018-06-30</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20325174427256</td>\n",
       "      <td>GH0</td>\n",
       "      <td>GAS</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(156414999216, 4464803701784)</td>\n",
       "      <td>156414999216</td>\n",
       "      <td>4464803701784</td>\n",
       "      <td>2018-07-01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20325174427256</td>\n",
       "      <td>GH0</td>\n",
       "      <td>GAS</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(156414999216, 4464803701784)</td>\n",
       "      <td>156414999216</td>\n",
       "      <td>4464803701784</td>\n",
       "      <td>2018-07-02</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20325174427256</td>\n",
       "      <td>GH0</td>\n",
       "      <td>GAS</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(156414999216, 4464803701784)</td>\n",
       "      <td>156414999216</td>\n",
       "      <td>4464803701784</td>\n",
       "      <td>2018-07-03</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20325174427256</td>\n",
       "      <td>GH0</td>\n",
       "      <td>GAS</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(156414999216, 4464803701784)</td>\n",
       "      <td>156414999216</td>\n",
       "      <td>4464803701784</td>\n",
       "      <td>2018-07-04</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20325174427256</td>\n",
       "      <td>GH0</td>\n",
       "      <td>GAS</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              ID    DACCOUNTID       DMETERNO         Dt  Use  \\\n",
       "0  (156414999216, 4464803701784)  156414999216  4464803701784 2018-06-30  0.0   \n",
       "1  (156414999216, 4464803701784)  156414999216  4464803701784 2018-07-01  0.0   \n",
       "2  (156414999216, 4464803701784)  156414999216  4464803701784 2018-07-02  0.0   \n",
       "3  (156414999216, 4464803701784)  156414999216  4464803701784 2018-07-03  0.0   \n",
       "4  (156414999216, 4464803701784)  156414999216  4464803701784 2018-07-04  0.0   \n",
       "\n",
       "      DCUSTOMERID TARIFF FUELTYPE  REVENUCODE  Weekday  Holiday  \n",
       "0  20325174427256    GH0      GAS        12.0        0        0  \n",
       "1  20325174427256    GH0      GAS        12.0        0        0  \n",
       "2  20325174427256    GH0      GAS        12.0        1        0  \n",
       "3  20325174427256    GH0      GAS        12.0        1        0  \n",
       "4  20325174427256    GH0      GAS        12.0        1        1  "
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "daily_use.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_to_plot = daily_use.groupby('ID').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5109 records have >=600 dates\n",
      "5108 records have >=601 dates\n",
      "5104 records have >=602 dates\n",
      "5099 records have >=603 dates\n",
      "5096 records have >=604 dates\n",
      "5089 records have >=605 dates\n",
      "5086 records have >=606 dates\n",
      "5081 records have >=607 dates\n",
      "5078 records have >=608 dates\n",
      "5072 records have >=609 dates\n",
      "5068 records have >=610 dates\n",
      "5065 records have >=611 dates\n",
      "5061 records have >=612 dates\n",
      "5057 records have >=613 dates\n",
      "5056 records have >=614 dates\n",
      "5053 records have >=615 dates\n",
      "5051 records have >=616 dates\n",
      "5045 records have >=617 dates\n",
      "5043 records have >=618 dates\n",
      "5038 records have >=619 dates\n",
      "5038 records have >=620 dates\n",
      "5034 records have >=621 dates\n",
      "5029 records have >=622 dates\n",
      "5025 records have >=623 dates\n",
      "5020 records have >=624 dates\n",
      "5017 records have >=625 dates\n",
      "5014 records have >=626 dates\n",
      "5013 records have >=627 dates\n",
      "5007 records have >=628 dates\n",
      "5005 records have >=629 dates\n",
      "5003 records have >=630 dates\n",
      "4988 records have >=631 dates\n",
      "4940 records have >=632 dates\n",
      "4890 records have >=633 dates\n",
      "4621 records have >=634 dates\n",
      "3776 records have >=635 dates\n",
      "102 records have >=636 dates\n",
      "39 records have >=637 dates\n",
      "32 records have >=638 dates\n",
      "32 records have >=639 dates\n",
      "32 records have >=640 dates\n",
      "32 records have >=641 dates\n",
      "32 records have >=642 dates\n",
      "32 records have >=643 dates\n",
      "32 records have >=644 dates\n",
      "32 records have >=645 dates\n",
      "32 records have >=646 dates\n",
      "32 records have >=647 dates\n",
      "32 records have >=648 dates\n",
      "32 records have >=649 dates\n",
      "32 records have >=650 dates\n",
      "32 records have >=651 dates\n",
      "32 records have >=652 dates\n",
      "32 records have >=653 dates\n",
      "32 records have >=654 dates\n",
      "32 records have >=655 dates\n",
      "32 records have >=656 dates\n",
      "32 records have >=657 dates\n",
      "32 records have >=658 dates\n",
      "32 records have >=659 dates\n",
      "32 records have >=660 dates\n",
      "32 records have >=661 dates\n",
      "32 records have >=662 dates\n",
      "32 records have >=663 dates\n",
      "32 records have >=664 dates\n",
      "32 records have >=665 dates\n",
      "32 records have >=666 dates\n",
      "32 records have >=667 dates\n",
      "32 records have >=668 dates\n",
      "32 records have >=669 dates\n",
      "32 records have >=670 dates\n",
      "32 records have >=671 dates\n",
      "32 records have >=672 dates\n",
      "32 records have >=673 dates\n",
      "32 records have >=674 dates\n",
      "32 records have >=675 dates\n",
      "32 records have >=676 dates\n",
      "32 records have >=677 dates\n",
      "32 records have >=678 dates\n",
      "32 records have >=679 dates\n",
      "32 records have >=680 dates\n",
      "32 records have >=681 dates\n",
      "32 records have >=682 dates\n",
      "32 records have >=683 dates\n",
      "32 records have >=684 dates\n",
      "32 records have >=685 dates\n",
      "32 records have >=686 dates\n",
      "32 records have >=687 dates\n",
      "32 records have >=688 dates\n",
      "32 records have >=689 dates\n",
      "32 records have >=690 dates\n",
      "32 records have >=691 dates\n",
      "32 records have >=692 dates\n",
      "32 records have >=693 dates\n",
      "32 records have >=694 dates\n",
      "32 records have >=695 dates\n",
      "32 records have >=696 dates\n",
      "32 records have >=697 dates\n",
      "32 records have >=698 dates\n",
      "32 records have >=699 dates\n"
     ]
    }
   ],
   "source": [
    "# for i in range(600,700):\n",
    "#    print(f\"{ids_to_plot[ids_to_plot['Use'] >= i]['Use'].count()} records have >={i} dates\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70.49457517260815 percent of IDs have at least 630 days of data\n"
     ]
    }
   ],
   "source": [
    "# find IDs with at least 630 days of data\n",
    "sufficient_ids = ids_to_plot[ids_to_plot['Use'] >= 630].reset_index()['ID']\n",
    "print(f\"{100*len(sufficient_ids)/len(daily_use['ID'].unique())} percent of IDs have at least 630 days of data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2017-02-25T00:00:00.000000000' '2017-02-26T00:00:00.000000000'\n",
      " '2017-02-27T00:00:00.000000000' '2017-02-28T00:00:00.000000000'\n",
      " '2017-03-01T00:00:00.000000000' '2017-03-02T00:00:00.000000000'\n",
      " '2017-03-03T00:00:00.000000000' '2017-03-04T00:00:00.000000000'\n",
      " '2017-03-05T00:00:00.000000000' '2017-03-06T00:00:00.000000000'\n",
      " '2017-03-07T00:00:00.000000000' '2017-03-08T00:00:00.000000000'\n",
      " '2017-03-09T00:00:00.000000000' '2017-03-10T00:00:00.000000000'\n",
      " '2017-03-11T00:00:00.000000000' '2017-03-12T00:00:00.000000000'\n",
      " '2017-03-13T00:00:00.000000000' '2017-03-14T00:00:00.000000000'\n",
      " '2017-03-15T00:00:00.000000000' '2017-03-16T00:00:00.000000000'\n",
      " '2017-03-17T00:00:00.000000000' '2017-03-18T00:00:00.000000000'\n",
      " '2017-03-19T00:00:00.000000000' '2017-03-20T00:00:00.000000000'\n",
      " '2017-03-21T00:00:00.000000000' '2017-03-22T00:00:00.000000000'\n",
      " '2017-03-23T00:00:00.000000000' '2017-03-24T00:00:00.000000000'\n",
      " '2017-03-25T00:00:00.000000000' '2017-03-26T00:00:00.000000000'\n",
      " '2017-03-27T00:00:00.000000000' '2017-03-28T00:00:00.000000000'\n",
      " '2017-03-29T00:00:00.000000000' '2017-03-30T00:00:00.000000000'\n",
      " '2017-03-31T00:00:00.000000000' '2017-04-01T00:00:00.000000000'\n",
      " '2017-04-02T00:00:00.000000000' '2017-04-03T00:00:00.000000000'\n",
      " '2017-01-01T00:00:00.000000000' '2017-01-02T00:00:00.000000000'\n",
      " '2017-01-03T00:00:00.000000000' '2017-01-04T00:00:00.000000000'\n",
      " '2017-01-05T00:00:00.000000000' '2017-01-06T00:00:00.000000000'\n",
      " '2017-01-07T00:00:00.000000000' '2017-01-08T00:00:00.000000000'\n",
      " '2017-01-09T00:00:00.000000000' '2017-01-10T00:00:00.000000000'\n",
      " '2017-01-11T00:00:00.000000000' '2017-01-12T00:00:00.000000000'\n",
      " '2017-01-13T00:00:00.000000000' '2017-01-14T00:00:00.000000000'\n",
      " '2017-01-15T00:00:00.000000000' '2017-01-16T00:00:00.000000000'\n",
      " '2017-01-17T00:00:00.000000000' '2017-01-18T00:00:00.000000000'\n",
      " '2017-01-19T00:00:00.000000000' '2017-01-20T00:00:00.000000000'\n",
      " '2017-01-21T00:00:00.000000000' '2017-01-22T00:00:00.000000000'\n",
      " '2017-01-23T00:00:00.000000000' '2017-01-24T00:00:00.000000000'\n",
      " '2017-01-25T00:00:00.000000000' '2017-01-26T00:00:00.000000000'\n",
      " '2017-01-27T00:00:00.000000000' '2017-01-28T00:00:00.000000000'\n",
      " '2017-01-29T00:00:00.000000000' '2017-01-30T00:00:00.000000000'\n",
      " '2017-01-31T00:00:00.000000000' '2017-02-01T00:00:00.000000000'\n",
      " '2017-02-02T00:00:00.000000000' '2017-02-03T00:00:00.000000000'\n",
      " '2017-02-04T00:00:00.000000000' '2017-02-05T00:00:00.000000000'\n",
      " '2017-02-06T00:00:00.000000000' '2017-02-07T00:00:00.000000000'\n",
      " '2017-02-08T00:00:00.000000000' '2017-02-09T00:00:00.000000000'\n",
      " '2017-02-10T00:00:00.000000000' '2017-02-11T00:00:00.000000000'\n",
      " '2017-02-12T00:00:00.000000000' '2017-02-13T00:00:00.000000000'\n",
      " '2017-02-14T00:00:00.000000000' '2017-02-15T00:00:00.000000000'\n",
      " '2017-02-16T00:00:00.000000000' '2017-02-17T00:00:00.000000000'\n",
      " '2017-02-18T00:00:00.000000000' '2017-02-19T00:00:00.000000000'\n",
      " '2017-02-20T00:00:00.000000000' '2017-02-21T00:00:00.000000000'\n",
      " '2017-02-22T00:00:00.000000000' '2017-02-23T00:00:00.000000000'\n",
      " '2017-02-24T00:00:00.000000000' '2016-10-01T00:00:00.000000000'\n",
      " '2016-10-02T00:00:00.000000000' '2016-10-03T00:00:00.000000000'\n",
      " '2016-10-04T00:00:00.000000000' '2016-10-05T00:00:00.000000000'\n",
      " '2016-10-06T00:00:00.000000000' '2016-10-07T00:00:00.000000000'\n",
      " '2016-10-08T00:00:00.000000000' '2016-10-09T00:00:00.000000000'\n",
      " '2016-10-10T00:00:00.000000000' '2016-10-11T00:00:00.000000000'\n",
      " '2016-10-12T00:00:00.000000000' '2016-10-13T00:00:00.000000000'\n",
      " '2016-10-14T00:00:00.000000000' '2016-10-15T00:00:00.000000000'\n",
      " '2016-10-16T00:00:00.000000000' '2016-10-17T00:00:00.000000000'\n",
      " '2016-10-18T00:00:00.000000000' '2016-10-19T00:00:00.000000000'\n",
      " '2016-10-20T00:00:00.000000000' '2016-10-21T00:00:00.000000000'\n",
      " '2016-10-22T00:00:00.000000000' '2016-10-23T00:00:00.000000000'\n",
      " '2016-10-24T00:00:00.000000000' '2016-10-25T00:00:00.000000000'\n",
      " '2016-10-26T00:00:00.000000000' '2016-10-27T00:00:00.000000000'\n",
      " '2016-10-28T00:00:00.000000000' '2016-10-29T00:00:00.000000000'\n",
      " '2016-10-30T00:00:00.000000000' '2016-10-31T00:00:00.000000000'\n",
      " '2016-11-01T00:00:00.000000000' '2016-11-02T00:00:00.000000000'\n",
      " '2016-11-03T00:00:00.000000000' '2016-11-04T00:00:00.000000000'\n",
      " '2016-11-05T00:00:00.000000000' '2016-11-06T00:00:00.000000000'\n",
      " '2016-11-07T00:00:00.000000000' '2016-11-08T00:00:00.000000000'\n",
      " '2016-11-09T00:00:00.000000000' '2016-11-10T00:00:00.000000000'\n",
      " '2016-11-11T00:00:00.000000000' '2016-11-12T00:00:00.000000000'\n",
      " '2016-11-13T00:00:00.000000000' '2016-11-14T00:00:00.000000000'\n",
      " '2016-11-15T00:00:00.000000000' '2016-11-16T00:00:00.000000000'\n",
      " '2016-11-17T00:00:00.000000000' '2016-11-18T00:00:00.000000000'\n",
      " '2016-11-19T00:00:00.000000000' '2016-11-20T00:00:00.000000000'\n",
      " '2016-11-21T00:00:00.000000000' '2016-11-22T00:00:00.000000000'\n",
      " '2016-11-23T00:00:00.000000000' '2016-11-24T00:00:00.000000000'\n",
      " '2016-11-25T00:00:00.000000000' '2016-11-26T00:00:00.000000000'\n",
      " '2016-11-27T00:00:00.000000000' '2016-11-28T00:00:00.000000000'\n",
      " '2016-11-29T00:00:00.000000000' '2016-11-30T00:00:00.000000000'\n",
      " '2016-12-01T00:00:00.000000000' '2016-12-02T00:00:00.000000000'\n",
      " '2016-12-03T00:00:00.000000000' '2016-12-04T00:00:00.000000000'\n",
      " '2016-12-05T00:00:00.000000000' '2016-12-06T00:00:00.000000000'\n",
      " '2016-12-07T00:00:00.000000000' '2016-12-08T00:00:00.000000000'\n",
      " '2016-12-09T00:00:00.000000000' '2016-12-10T00:00:00.000000000'\n",
      " '2016-12-11T00:00:00.000000000' '2016-12-12T00:00:00.000000000'\n",
      " '2016-12-13T00:00:00.000000000' '2016-12-14T00:00:00.000000000'\n",
      " '2016-12-15T00:00:00.000000000' '2016-12-16T00:00:00.000000000'\n",
      " '2016-12-17T00:00:00.000000000' '2016-12-18T00:00:00.000000000'\n",
      " '2016-12-19T00:00:00.000000000' '2016-12-20T00:00:00.000000000'\n",
      " '2016-12-21T00:00:00.000000000' '2016-12-22T00:00:00.000000000'\n",
      " '2016-12-23T00:00:00.000000000' '2016-12-24T00:00:00.000000000'\n",
      " '2016-12-25T00:00:00.000000000' '2016-12-26T00:00:00.000000000'\n",
      " '2016-12-27T00:00:00.000000000' '2016-12-28T00:00:00.000000000'\n",
      " '2016-12-29T00:00:00.000000000' '2016-12-30T00:00:00.000000000'\n",
      " '2016-12-31T00:00:00.000000000']\n"
     ]
    }
   ],
   "source": [
    "print(daily_use[daily_use['ID'].isin(sufficient_ids)]['Dt'].dt.date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The IDs with sufficient data overlap on 309 days of the most recent year of data\n"
     ]
    }
   ],
   "source": [
    "# find the dates for which sufficient_ids all have data\n",
    "pivoted = (daily_use[daily_use['ID'].isin(sufficient_ids)]\n",
    "           .pivot_table(index='Dt', columns=['ID'], values='Use', fill_value=None))\n",
    "sufficient_dates = pivoted.dropna().reset_index()['Dt']\n",
    "del pivoted\n",
    "print(f\"The IDs with sufficient data overlap on {len(sufficient_dates)} days of the data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      2017-10-12\n",
      "1      2017-10-13\n",
      "2      2017-10-14\n",
      "3      2017-10-16\n",
      "4      2017-10-17\n",
      "5      2017-10-18\n",
      "6      2017-10-19\n",
      "7      2017-10-21\n",
      "8      2017-10-22\n",
      "9      2017-10-23\n",
      "10     2017-10-24\n",
      "11     2017-10-25\n",
      "12     2017-10-31\n",
      "13     2017-11-01\n",
      "14     2017-11-02\n",
      "15     2017-11-03\n",
      "16     2017-11-04\n",
      "17     2017-11-05\n",
      "18     2017-11-06\n",
      "19     2017-11-07\n",
      "20     2017-11-08\n",
      "21     2017-11-09\n",
      "22     2017-11-12\n",
      "23     2017-11-13\n",
      "24     2017-11-14\n",
      "25     2017-11-15\n",
      "26     2017-11-16\n",
      "27     2017-11-17\n",
      "28     2017-11-18\n",
      "29     2017-11-19\n",
      "          ...    \n",
      "279    2018-08-17\n",
      "280    2018-08-22\n",
      "281    2018-08-23\n",
      "282    2018-08-24\n",
      "283    2018-08-25\n",
      "284    2018-08-26\n",
      "285    2018-08-27\n",
      "286    2018-08-29\n",
      "287    2018-08-30\n",
      "288    2018-08-31\n",
      "289    2018-09-01\n",
      "290    2018-09-07\n",
      "291    2018-09-08\n",
      "292    2018-09-09\n",
      "293    2018-09-10\n",
      "294    2018-09-13\n",
      "295    2018-09-14\n",
      "296    2018-09-15\n",
      "297    2018-09-16\n",
      "298    2018-09-17\n",
      "299    2018-09-18\n",
      "300    2018-09-19\n",
      "301    2018-09-20\n",
      "302    2018-09-21\n",
      "303    2018-09-22\n",
      "304    2018-09-23\n",
      "305    2018-09-24\n",
      "306    2018-09-25\n",
      "307    2018-09-26\n",
      "308    2018-09-27\n",
      "Name: Dt, Length: 309, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(sufficient_dates.dt.date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter total use dataset to contain only IDs who have sufficient data and only dates where sufficient_ids have data\n",
    "sufficient_use = daily_use[(daily_use['ID'].isin(sufficient_ids)) & (daily_use['Dt'].isin(sufficient_dates))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save file(s) as pickles\n",
    "# using save location to save with other data files outside of git repo\n",
    "# data.to_csv(location+'peco.csv', sep='\\t')\n",
    "daily_use.to_pickle(location+'fcast_daily.pkl.zip')\n",
    "sufficient_use.to_pickle(location+'fcast_sufficient_daily.pkl.zip')\n",
    "ids.to_pickle(location+'fcast_daily_ids.pkl.zip')\n",
    "sufficient_ids.to_pickle(location+'fcast_sufficient_daily_ids.pkl.zip')\n",
    "del (daily_use,sufficient_use,ids,sufficient_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make hourly data###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append rate code data\n",
    "tidy_use = tidy_use.merge(customer, how='inner', on=['ID','DACCOUNTID','DMETERNO'])\n",
    "#tidy_use.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create single column for date and time\n",
    "tidy_use['year'] = pd.to_datetime(tidy_use['Dt'].values).year\n",
    "tidy_use['month'] = pd.to_datetime(tidy_use['Dt'].values).month\n",
    "tidy_use['day'] = pd.to_datetime(tidy_use['Dt'].values).day\n",
    "tidy_use['datetime'] = pd.to_datetime(tidy_use[['year','month','day','Hour']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add dummy variables for day-of-week\n",
    "#use = use.join(pd.get_dummies(use['Dt'].dt.weekday_name))\n",
    "tidy_use['Weekday'] = pd.get_dummies(tidy_use['Dt'].dt.weekday < 5)[True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add dummy variables for hour-of-day\n",
    "tidy_use = tidy_use.join(pd.get_dummies(tidy_use['Hour'],prefix='HR'))\n",
    "\n",
    "# Add dummy variables for part-of-day\n",
    "tidy_use['MORN'] = pd.get_dummies((tidy_use['Hour'] >= 5) & (tidy_use['Hour'] < 9))[True] # 5-9 am\n",
    "tidy_use['DAY'] = pd.get_dummies((tidy_use['Hour'] >= 9) & (tidy_use['Hour'] < 17))[True] # 9 am-5 pm\n",
    "tidy_use['EVE'] = pd.get_dummies((tidy_use['Hour'] >= 17) & (tidy_use['Hour'] < 21))[True] # 5-10 pm\n",
    "tidy_use['NIGHT'] = pd.get_dummies((tidy_use['Hour'] < 5) | (tidy_use['Hour'] >= 21) )[True] # 10 pm-6 am\n",
    "# tidy_use.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add dummy variables for holidays\n",
    "from pandas.tseries.holiday import USFederalHolidayCalendar\n",
    "cal = USFederalHolidayCalendar()\n",
    "#dr = pd.date_range(start=min(tidy_use['Dt']), end=max(tidy_use['Dt']))\n",
    "dr = pd.date_range(start=\"1-1-2010\", end=\"12-31-2020\")\n",
    "holidays = cal.holidays(start=dr.min(), end=dr.max())\n",
    "\n",
    "tidy_use['Holiday'] = pd.get_dummies(tidy_use['Dt'].isin(holidays))[True]\n",
    "del (USFederalHolidayCalendar, cal, dr, holidays)\n",
    "#tidy_use.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort by date & time\n",
    "tidy_use = tidy_use.drop(columns=['Dt','year','month','day','Hour'])\n",
    "tidy_use = tidy_use.sort_values(by=['datetime'])\n",
    "tidy_use = tidy_use.rename(columns={'datetime':'Dt'}) # for consistency\n",
    "# tidy_use.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_to_plot = tidy_use.groupby('ID').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5109 records have >=600 dates\n",
      "5108 records have >=601 dates\n",
      "5104 records have >=602 dates\n",
      "5099 records have >=603 dates\n",
      "5096 records have >=604 dates\n",
      "5089 records have >=605 dates\n",
      "5086 records have >=606 dates\n",
      "5081 records have >=607 dates\n",
      "5078 records have >=608 dates\n",
      "5072 records have >=609 dates\n",
      "5068 records have >=610 dates\n",
      "5065 records have >=611 dates\n",
      "5061 records have >=612 dates\n",
      "5057 records have >=613 dates\n",
      "5056 records have >=614 dates\n",
      "5053 records have >=615 dates\n",
      "5051 records have >=616 dates\n",
      "5045 records have >=617 dates\n",
      "5043 records have >=618 dates\n",
      "5038 records have >=619 dates\n",
      "5038 records have >=620 dates\n",
      "5034 records have >=621 dates\n",
      "5029 records have >=622 dates\n",
      "5025 records have >=623 dates\n",
      "5020 records have >=624 dates\n",
      "5017 records have >=625 dates\n",
      "5014 records have >=626 dates\n",
      "5013 records have >=627 dates\n",
      "5007 records have >=628 dates\n",
      "5005 records have >=629 dates\n",
      "5003 records have >=630 dates\n",
      "4988 records have >=631 dates\n",
      "4940 records have >=632 dates\n",
      "4890 records have >=633 dates\n",
      "4621 records have >=634 dates\n",
      "3776 records have >=635 dates\n",
      "102 records have >=636 dates\n",
      "39 records have >=637 dates\n",
      "32 records have >=638 dates\n",
      "32 records have >=639 dates\n",
      "32 records have >=640 dates\n",
      "32 records have >=641 dates\n",
      "32 records have >=642 dates\n",
      "32 records have >=643 dates\n",
      "32 records have >=644 dates\n",
      "32 records have >=645 dates\n",
      "32 records have >=646 dates\n",
      "32 records have >=647 dates\n",
      "32 records have >=648 dates\n",
      "32 records have >=649 dates\n",
      "32 records have >=650 dates\n",
      "32 records have >=651 dates\n",
      "32 records have >=652 dates\n",
      "32 records have >=653 dates\n",
      "32 records have >=654 dates\n",
      "32 records have >=655 dates\n",
      "32 records have >=656 dates\n",
      "32 records have >=657 dates\n",
      "32 records have >=658 dates\n",
      "32 records have >=659 dates\n",
      "32 records have >=660 dates\n",
      "32 records have >=661 dates\n",
      "32 records have >=662 dates\n",
      "32 records have >=663 dates\n",
      "32 records have >=664 dates\n",
      "32 records have >=665 dates\n",
      "32 records have >=666 dates\n",
      "32 records have >=667 dates\n",
      "32 records have >=668 dates\n",
      "32 records have >=669 dates\n",
      "32 records have >=670 dates\n",
      "32 records have >=671 dates\n",
      "32 records have >=672 dates\n",
      "32 records have >=673 dates\n",
      "32 records have >=674 dates\n",
      "32 records have >=675 dates\n",
      "32 records have >=676 dates\n",
      "32 records have >=677 dates\n",
      "32 records have >=678 dates\n",
      "32 records have >=679 dates\n",
      "32 records have >=680 dates\n",
      "32 records have >=681 dates\n",
      "32 records have >=682 dates\n",
      "32 records have >=683 dates\n",
      "32 records have >=684 dates\n",
      "32 records have >=685 dates\n",
      "32 records have >=686 dates\n",
      "32 records have >=687 dates\n",
      "32 records have >=688 dates\n",
      "32 records have >=689 dates\n",
      "32 records have >=690 dates\n",
      "32 records have >=691 dates\n",
      "32 records have >=692 dates\n",
      "32 records have >=693 dates\n",
      "32 records have >=694 dates\n",
      "32 records have >=695 dates\n",
      "32 records have >=696 dates\n",
      "32 records have >=697 dates\n",
      "32 records have >=698 dates\n",
      "32 records have >=699 dates\n"
     ]
    }
   ],
   "source": [
    "# for i in range(600,700):\n",
    "#    print(f\"{ids_to_plot[ids_to_plot['Use'] >= i*24]['Use'].count()} records have >={i} dates\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65.11201916302663 percent of IDs have at least 634 days of data\n"
     ]
    }
   ],
   "source": [
    "# find IDs with at least 630 days of data\n",
    "sufficient_ids = ids_to_plot[ids_to_plot['Use'] >= 630*24].reset_index()['ID']\n",
    "print(f\"{100*len(sufficient_ids)/len(tidy_use['ID'].unique())} percent of IDs have at least 630 days of data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The IDs with sufficient data overlap on 332.0 days of the most recent year of data\n"
     ]
    }
   ],
   "source": [
    "# find the dates for which sufficient_ids all have data\n",
    "pivoted = (tidy_use[tidy_use['ID'].isin(sufficient_ids)]\n",
    "           .pivot_table(index='Dt', columns=['ID'], values='Use', fill_value=None))\n",
    "sufficient_dates = pivoted.dropna().reset_index()['Dt']\n",
    "del pivoted\n",
    "print(f\"The IDs with sufficient data overlap on {len(sufficient_dates)/24} days of the most recent year of data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter total use dataset to contain only IDs who have sufficient data and only dates where sufficient_ids have data\n",
    "sufficient_use = tidy_use[(tidy_use['ID'].isin(sufficient_ids)) & (tidy_use['Dt'].isin(sufficient_dates))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2016-10-01 00:00:00')"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(tidy_use['Dt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save file(s) as pickles\n",
    "# using save location to save with other data files outside of git repo\n",
    "# data.to_csv(location+'peco.csv', sep='\\t')\n",
    "tidy_use.to_pickle(location+'fcast_hourly.pkl.zip')\n",
    "sufficient_use.to_pickle(location+'fcast_sufficient_hourly.pkl.zip')\n",
    "ids.to_pickle(location+'fcast_daily_ids.pkl.zip')\n",
    "sufficient_ids.to_pickle(location+'fcast_sufficient_hourly_ids.pkl.zip')\n",
    "del (tidy_use,sufficient_use,ids,sufficient_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Merge use & weather? ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "hourly = pd.read_pickle(location+'peco_hourly.pkl.zip')\n",
    "sufficient_hourly = pd.read_pickle(location+'peco_sufficient_hourly.pkl.zip')\n",
    "weather = pd.read_pickle(location+'hourly_weather.pkl.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge\n",
    "hourly = pd.merge(hourly, weather, how='inner', on=['Dt'])\n",
    "sufficient_hourly = pd.merge(sufficient_hourly, weather, how='inner', on=['Dt'])\n",
    "# CustIDs || Date | Consumption |||| Weather_variables "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

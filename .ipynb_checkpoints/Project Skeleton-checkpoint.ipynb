{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PROJECT SKELETON #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning & Checking "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* import\n",
    "* sample size\n",
    "* % missing\n",
    "* outliers\n",
    "* distributions(?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data\n",
    "location = '/Users/mithras/Documents/_SCHOOL/_Drexel/BUSN 710 - Capstone/Data/Forecasting Project/'\n",
    "\n",
    "use_in = pd.read_excel(location+'Sample Usage Query Drexel 2018.09.27.xlsx')\n",
    "\n",
    "temp_in = pd.read_excel(location+'Weather Data for Drexel 9_28_2018.xlsx', sheet_name=\"TMP\")\n",
    "humid_in = pd.read_excel(location+'Weather Data for Drexel 9_28_2018.xlsx', sheet_name=\"HUM\")\n",
    "wind_in = pd.read_excel(location+'Weather Data for Drexel 9_28_2018.xlsx', sheet_name=\"WSP\")\n",
    "cloud_in = pd.read_excel(location+'Weather Data for Drexel 9_28_2018.xlsx', sheet_name=\"CC\")\n",
    "\n",
    "customer_in = pd.read_excel(location+'Plain Customer Drexel 2018.09.27.xlsx')\n",
    "\n",
    "ratecode_in = pd.read_excel(location+'Rate Codes for Drexel 9_28_2018.xlsx')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean\n",
    "use = use_in.drop(columns=['ZIPCODE','INTERVAL','UOM','DAILY_INTERVAL_USAGE'])\n",
    "temp = temp_in.drop(columns=['Avg','HighDB','LowDB','AvgHL','Gas Day Average','HDD-HL','CDD-HL','HDD-24','CDD-24'])\n",
    "humid = humid_in.drop(columns=['Avg','Unnamed: 26','Unnamed: 27','Unnamed: 28'])\n",
    "wind = wind_in.drop(columns=['Avg','Unnamed: 26'])\n",
    "cloud = cloud_in.drop(columns=['AvgDaytime','Avg'])\n",
    "\n",
    "# convert to datetime\n",
    "use['METERREADDATE'] =  pd.to_datetime(use['METERREADDATE'])\n",
    "temp['Dt'] =  pd.to_datetime(temp['Dt'])\n",
    "humid['Dt'] =  pd.to_datetime(humid['Dt'])\n",
    "wind['Dt'] =  pd.to_datetime(wind['Dt'])\n",
    "cloud['Dt'] =  pd.to_datetime(cloud['Dt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename for consistency\n",
    "def decrement(x, startswith, split):\n",
    "    \"\"\"\n",
    "    decrements a passed string of form \"demo#\" by 1\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x : string to be decremented\n",
    "    split : string to split on\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    y : decremented string\n",
    "    \"\"\"\n",
    "    if x.startswith(startswith):\n",
    "        a,b = x.split(split)\n",
    "        b = int(b)-1\n",
    "        y = a + split + str(b)\n",
    "\n",
    "        return y\n",
    "\n",
    "    else:\n",
    "        return x\n",
    "    \n",
    "use = use.rename(columns=lambda x: decrement(x, 'INTERVAL_', \"_\"))\n",
    "use = use.rename(columns=lambda x: x.replace('INTERVAL_', 'HR'))\n",
    "# use = use.drop(column=['HR24'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Restructuring ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tidy / Stack data (transform into tall data - one row per customer per hour):\n",
    "# ref: http://www.jeannicholashould.com/tidy-data-in-python.html\n",
    "tidy_use = pd.melt(use, \n",
    "                   id_vars=['ID','DAccountID','DMeterNo','METERREADDATE'],\n",
    "                   var_name='Hour', value_name='Use')\n",
    "\n",
    "tidy_temp = pd.melt(temp, \n",
    "                    id_vars=['Dt'],\n",
    "                    var_name='Hour', value_name='Temp')\n",
    "\n",
    "tidy_humid = pd.melt(temp,\n",
    "                     id_vars=['Dt'],\n",
    "                     var_name='Hour', value_name='Humid')\n",
    "\n",
    "tidy_wind = pd.melt(temp, \n",
    "                    id_vars=['Dt'],\n",
    "                    var_name='Hour', value_name='Wind')\n",
    "\n",
    "tidy_cloud = pd.melt(temp, \n",
    "                     id_vars=['Dt'],\n",
    "                     var_name='Hour', value_name='Cloud')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot convert float NaN to integer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-212-72c72a206fc0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# relabel & retype for sorting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtidy_use\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Hour'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtidy_use\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Hour'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'(\\d+)'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtidy_temp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Hour'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtidy_temp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Hour'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'(\\d+)'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mtidy_humid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Hour'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtidy_humid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Hour'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'(\\d+)'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtidy_wind\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Hour'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtidy_wind\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Hour'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'(\\d+)'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    176\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_deprecate_kwarg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mastype\u001b[0;34m(self, dtype, copy, errors, **kwargs)\u001b[0m\n\u001b[1;32m   4999\u001b[0m             \u001b[0;31m# else, only a single dtype is given\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5000\u001b[0m             new_data = self._data.astype(dtype=dtype, copy=copy, errors=errors,\n\u001b[0;32m-> 5001\u001b[0;31m                                          **kwargs)\n\u001b[0m\u001b[1;32m   5002\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_constructor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__finalize__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5003\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mastype\u001b[0;34m(self, dtype, **kwargs)\u001b[0m\n\u001b[1;32m   3712\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3713\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3714\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'astype'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3715\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3716\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, f, axes, filter, do_integrity_check, consolidate, **kwargs)\u001b[0m\n\u001b[1;32m   3579\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3580\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mgr'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3581\u001b[0;31m             \u001b[0mapplied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3582\u001b[0m             \u001b[0mresult_blocks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_extend_blocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapplied\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult_blocks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3583\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mastype\u001b[0;34m(self, dtype, copy, errors, values, **kwargs)\u001b[0m\n\u001b[1;32m    573\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'raise'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m         return self._astype(dtype, copy=copy, errors=errors, values=values,\n\u001b[0;32m--> 575\u001b[0;31m                             **kwargs)\n\u001b[0m\u001b[1;32m    576\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m     def _astype(self, dtype, copy=False, errors='raise', values=None,\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36m_astype\u001b[0;34m(self, dtype, copy, errors, values, klass, mgr, **kwargs)\u001b[0m\n\u001b[1;32m    662\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    663\u001b[0m                 \u001b[0;31m# _astype_nansafe works fine with 1-d only\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 664\u001b[0;31m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mastype_nansafe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    665\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/pandas/core/dtypes/cast.py\u001b[0m in \u001b[0;36mastype_nansafe\u001b[0;34m(arr, dtype, copy)\u001b[0m\n\u001b[1;32m    707\u001b[0m         \u001b[0;31m# work around NumPy brokenness, #1987\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    708\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0missubdtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minteger\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 709\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype_intsafe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    710\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    711\u001b[0m         \u001b[0;31m# if we have a datetime/timedelta array of objects\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.astype_intsafe\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/src/util.pxd\u001b[0m in \u001b[0;36mutil.set_value_at_unsafe\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: cannot convert float NaN to integer"
     ]
    }
   ],
   "source": [
    "# relabel & retype for sorting\n",
    "tidy_use['Hour'] = tidy_use['Hour'].str.extract('(\\d+)').astype(int)\n",
    "tidy_temp['Hour'] = tidy_temp['Hour'].str.extract('(\\d+)').astype(int)\n",
    "tidy_humid['Hour'] = tidy_humid['Hour'].str.extract('(\\d+)').astype(int)\n",
    "tidy_wind['Hour'] = tidy_wind['Hour'].str.extract('(\\d+)').astype(int)\n",
    "tidy_cloud['Hour'] = tidy_cloud['Hour'].str.extract('(\\d+)').astype(int)\n",
    "\n",
    "# sort by date & time\n",
    "tidy_use = tidy_use.sort_values(by=[\"METERREADDATE\",\"Hour\"])\n",
    "tidy_temp = tidy_temp.sort_values(by=[\"Dt\",\"Hour\"])\n",
    "tidy_humid = tidy_humid.sort_values(by=[\"Dt\",\"Hour\"])\n",
    "tidy_wind = tidy_wind.sort_values(by=[\"Dt\",\"Hour\"])\n",
    "tidy_cloud = tidy_cloud.sort_values(by=[\"Dt\",\"Hour\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lagDeltas(data, colname, newname, lag):\n",
    "    \"\"\"Calculates specified lag of provided column and saves as a new column\n",
    "     \n",
    "    Parameters\n",
    "    ----------\n",
    "    data : pandas dataframe object to be used calculate lag 0 and lag 1 deltas\n",
    "    colname : name of column to be lagged\n",
    "    newname : name of result to be added to pandas dataframe\n",
    "    lag : number of lags to take \n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    lagData : pandas dataframe with new column\n",
    "    \"\"\"\n",
    "    \n",
    "    data[newname] = data[colname].shift(lag)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find lags of weather variables\n",
    "# Lag temp\n",
    "for i in range(1,7):\n",
    "    temp = lagDeltas(tidy_temp, \"Temp\", \"Temp\"+str(i), i)\n",
    "\n",
    "# Lag humid\n",
    "for i in range(1,7):\n",
    "    humid = lagDeltas(tidy_humid, \"Humid\", \"Humid\"+str(i), i)\n",
    "    \n",
    "# Lag wind\n",
    "for i in range(1,7):\n",
    "    wind = lagDeltas(tidy_wind, \"Wind\", \"Wind\"+str(i), i)\n",
    "\n",
    "# Lag cloud\n",
    "for i in range(1,7):\n",
    "    cloud = lagDeltas(tidy_cloud, \"Cloud\", \"Cloud\"+str(i), i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Are lags enough or do we want deltas?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge\n",
    "pd.merge(customer, weather, how = 'left')\n",
    "\n",
    "\n",
    "\n",
    "# CustID | Date | Time | Consumption |||| Weather_variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save file(s) as pickles?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression --> Pipeline ##\n",
    "We want to find a function f that that predicts gas consumption (y) based on weather data x1, x2, ..., xn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What type of regression?  Try using small sample and pick version that (a) minimizes error and (b) gives us small enough runtimes to be able to do it for each consumer ID:\n",
    "* Linear regression + LASSO\n",
    "* GLM/nonlinearn (polynomial)\n",
    "* GLM/nonlinear (poisson)\n",
    "* Kernel Ridge Regression (KRR)\n",
    "* SVR\n",
    "    * RBF\n",
    "    * Polynomial kernel\n",
    "\n",
    "How much data to use?  Last year?  All?  \n",
    "* What is runtime difference? \n",
    "* What is accuracy difference?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "parallel processing in python: \n",
    "https://stackoverflow.com/questions/18294534/is-there-a-foreach-function-in-python-3https://docs.python.org/3/library/multiprocessing.html\n",
    "https://joblib.readthedocs.io/en/latest/parallel.html\n",
    "see also: Python Data Science Essentials ~pg390"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear+LASSO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temp1\n",
      "temp2\n",
      "temp3\n",
      "temp4\n",
      "temp5\n",
      "temp6\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,7):\n",
    "    print(\"temp\"+str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GLM poly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GLM poisson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KRR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVR RBF\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.grid_search import RandomizedSearchCV\n",
    "\n",
    "\n",
    "hypothesis = SVR(kernel='rbf', random_state=101)\n",
    "search_dict = {'C': [0.01, 0.1, 1, 10, 100], \n",
    "'gamma': [0.1, 0.01, 0.001, 0.0001]}\n",
    "search_func = RandomizedSearchCV(estimator=hypothesis, \n",
    "param_distributions=search_dict, n_iter=10, scoring='accuracy',\n",
    "n_jobs=-1, iid=True, refit=True, cv=5, random_state=101)\n",
    "search_func.fit(X_train, y_train)\n",
    "\n",
    "print ('Best parameters %s' % search_func.best_params_)\n",
    "print ('Cross validation accuracy: mean = %0.3f' % search_func.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVR poly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parallelize!  For each ID:\n",
    "import multiprocessor\n",
    "import joblib\n",
    "\n",
    "# predict *gas* as a function of weather data\n",
    "    # how many lag-deltas for each weather variable? (hypothesis <= 3?)\n",
    "    # additive or multiplicative? (hypothesis: mult)\n",
    "    # degree polynomial (hypothesis = 3?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def errfn (acts, preds, method):\n",
    "    \"\"\"\n",
    "    Calculates error using a variety of functions, \n",
    "    inc. Root Mean Squared Error (RMSE), Mean Absolute Error (MAE), & Symmetric Mean Average Percent Error (SMAPE)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    preds : predicted values\n",
    "    method : error function, including RMSE, MAE, SMAPE\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    error : float\n",
    "\n",
    "    @author: Yi Zhu, Alex Graber\n",
    "    \"\"\"\n",
    "\n",
    "    method = method.upper()\n",
    "    \n",
    "    # param detection\n",
    "    if (method == \"RMSE\"): \n",
    "        #calculate RMSE\n",
    "        from math import sqrt\n",
    "        from sklearn.metrics import mean_squared_error\n",
    "        error = sqrt(mean_squared_error(acts, preds))\n",
    "    elif (method == \"MAE\"):\n",
    "        #calculate MAE\n",
    "        from sklearn.metrics import mean_absolute_error\n",
    "        error = mean_absolute_error(acts, preds)\n",
    "    elif (method == \"SMAPE\"):\n",
    "        #calculate SMAPE\n",
    "        from math import sqrt\n",
    "        numerator = \n",
    "        denominator = \n",
    "        error = numerator / denominator\n",
    "    else:\n",
    "        print(\"Attempted method not in [RMSE, MAE, SMAPE]\")\n",
    "        break\n",
    "\n",
    "    return error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new df with ID, regression weights, regression error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Sun Jun  3 20:17:26 2018\n",
    "\n",
    "@author: yizhu6\n",
    "\"\"\"\n",
    "\n",
    "# import W and reshape the data to get prepared, manually import\n",
    "nsamples, nx, ny = W_new.shape\n",
    "data_initial = W_new.reshape((nsamples,nx*ny))\n",
    "\n",
    "# remove zero obs\n",
    "df=pd.DataFrame(data_initial)\n",
    "df['total']= df.sum(axis=1)\n",
    "df_removezero = df[df.total != 0]\n",
    "data =np.array(df_removezero.drop(['total'],axis=1))\n",
    "\n",
    "#########################################\n",
    "\n",
    "# normalize\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "data_scaled = scaler.fit_transform(data)\n",
    "    \n",
    "# determine K using elbow method\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import metrics\n",
    "from scipy.spatial.distance import cdist\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# euclidean\n",
    "start_time = time.time()\n",
    "sse1 = []\n",
    "K = range(1,12)\n",
    "for k in K:\n",
    "    kmeanModel = KMeans(n_clusters=k).fit(data)\n",
    "    kmeanModel.fit(data)\n",
    "    sse1.append(sum(np.min(cdist(data, kmeanModel.cluster_centers_, 'euclidean'), axis=1)) / X.shape[0])\n",
    "\n",
    "# Plot the elbow\n",
    "plt.plot(K, sse1, 'bx-')\n",
    "plt.xlabel('Number of K')\n",
    "plt.ylabel('SSE')\n",
    "plt.title('The Elbow Method Showing the Optimal K (Euclidean)')\n",
    "plt.show()\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "# do clustering\n",
    "import nltk\n",
    "from nltk.cluster.kmeans import KMeansClusterer\n",
    "from sklearn.cluster import AgglomerativeClustering,KMeans\n",
    "\n",
    "n_clusters = 5\n",
    "\n",
    "#############################################\n",
    "\n",
    "#KMeans - Euclidean\n",
    "kclusterer = KMeansClusterer(n_clusters, distance=nltk.cluster.util.euclidean_distance)\n",
    "clusters_table = kclusterer.cluster(data, assign_clusters=True)\n",
    "pd.DataFrame(pd.Series(clusters_table).value_counts(), columns = ['NO. of clients']).T\n",
    "#                  9   5   7   0   4   3   1   6   2   10  8 \n",
    "#NO. of clients  3908  13   6   5   4   3   3   3   3   2   1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for (k=2 to n):\n",
    "    # kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scree plot\n",
    "\n",
    "# super awesome visualization and screen plot kernels I came across while doing the NYC taxi fare forcasting project:\n",
    "# https://www.kaggle.com/ashishpatel26/exploration-of-nyc\n",
    "# https://www.kaggle.com/willkoehrsen/a-walkthrough-and-a-challenge\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SSE \n",
    "from sklearn.cluster import KMeans\n",
    "sse = []\n",
    "K = range(1,20)\n",
    "for k in K:\n",
    "    kmeanModel = KMeans(n_clusters=k).fit(data)\n",
    "    kmeanModel.fit(data)\n",
    "    sse.append(sum(np.min(cdist(data, kmeanModel.cluster_centers_, 'cosine'), axis=1)) / X.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def: PCA\n",
    "    # normalize or not?\n",
    "\n",
    "pca_2cw = PCA(n_components=2, whiten=True)\n",
    "X_pca_1cw = pca_2cw.fit_transform(iris.data)\n",
    "plt.scatter(X_pca_1cw[:,0], X_pca_1cw[:,1], c=iris.target,\n",
    "alpha=0.8, s=60, marker='o', edgecolors='white')\n",
    "plt.show()\n",
    "pca_2cw.explained_variance_ratio_.sum() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize cluster identities using PCA on weight vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize rate-code identities using PCA on weight vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forecast ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def autoARIMA (train, bounds):\n",
    "    \"\"\"\n",
    "    Runs an automatic ARIMA on data, given bounds to speed grid search for p,d,q,P,D,Q params\n",
    "    \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    train : training data\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    forecast : model\n",
    "\n",
    "    @author: Yi Zhu, Alex Graber\n",
    "    \"\"\"\n",
    "    \n",
    "    #preprocessing (since arima takes univariate series as input)\n",
    "    train.drop('Month',axis=1,inplace=True)\n",
    "    valid.drop('Month',axis=1,inplace=True)\n",
    "\n",
    "\n",
    "    #building the model\n",
    "    from pyramid.arima import auto_arima\n",
    "    model = auto_arima(train, trace=True, error_action='ignore', suppress_warnings=True)\n",
    "    model.fit(train)\n",
    "\n",
    "    forecast = model.predict(n_periods=len(valid))\n",
    "    forecast = pd.DataFrame(forecast,index = valid.index,columns=['Prediction'])\n",
    "\n",
    "    return forecast\n",
    "\n",
    "\n",
    "\n",
    "    # links\n",
    "    # https://www.analyticsvidhya.com/blog/2018/08/auto-arima-time-series-modeling-python-r/\n",
    "    # https://machinelearningmastery.com/arima-for-time-series-forecasting-with-python/\n",
    "    # https://medium.com/@josemarcialportilla/using-python-and-auto-arima-to-forecast-seasonal-time-series-90877adff03c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forecast each cluster\n",
    "fcasts = []\n",
    "for (i in 1:k):\n",
    "    # subset data based on cluster ID\n",
    "    \n",
    "    \n",
    "    # split data into training and testing\n",
    "    ratio = 0.7\n",
    "    train = data[:int(ratio*(len(data)))]\n",
    "    test = data[int(ratio*(len(data))):]\n",
    "\n",
    "    # forecast\n",
    "    fcast[i]=autoARIMA(train, bounds)\n",
    "    \n",
    "    # calculate error\n",
    "    preds = fcast[i].predictions\n",
    "    print(errfn(test, preds, \"RMSE\"))\n",
    "    print(errfn(test, preds, \"SMAPE\"))\n",
    "\n",
    "# compare forecast to testing data\n",
    "plt.plot(train, label='Train')\n",
    "plt.plot(test, label='test')\n",
    "plt.plot(forecast, label='Prediction')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forecast each rate code\n",
    "fcasts = []\n",
    "for (i in 1:k):\n",
    "    # subset data based on rate code\n",
    "    \n",
    "    \n",
    "    # split data into training and testing\n",
    "    ratio = 0.7\n",
    "    train = data[:int(ratio*(len(data)))]\n",
    "    test = data[int(ratio*(len(data))):]\n",
    "\n",
    "    # forecast\n",
    "    fcast[i]=autoARIMA(train, bounds)\n",
    "    \n",
    "    # calculate error\n",
    "    preds = fcast[i].predictions\n",
    "    print(errfn(test, preds, \"RMSE\"))\n",
    "    print(errfn(test, preds, \"SMAPE\"))\n",
    "\n",
    "# compare forecast to testing data\n",
    "plt.plot(train, label='Train')\n",
    "plt.plot(test, label='test')\n",
    "plt.plot(forecast, label='Prediction')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segmentation comparison ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* do any of our clusters correspond with rate code segments?\n",
    "* if so, can we correct rate codes based on segments directly?\n",
    "* if not, can we come up with a classifier to estimate rate code based on segment?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# identify nearest neighbors\n",
    "# KNN: K=10, default measure of distance (euclidean)\n",
    "clf = KNeighborsClassifier(3)\n",
    "clf.fit(X_train, y_train)\n",
    "preds = clf.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print (classification_report(acts, preds))\n",
    "\n",
    "# identify mode of neighbor classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Archive ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cross correlation ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The proper way to compare for relationships between time series is by the cross-correlation function (*assuming stationarity*). Having the same length is not essential. \n",
    "The cross correlation at lag 0 just computes a correlation like doing the Pearson correlation estimate pairing the data at the identical time points. If they do have the same length as you are assuming, you will have exact T pairs where T is the number of time points for each series. \n",
    "Lag 1 cross correlation matches time t from series 1 with time t+1 in series 2. Note that here even though the series are the same length you only have T-2 pair as one point in the first series has no match in the second and one other point in the second series will not have a match from the first. Given these two series you can estimate the cross-correlation at several lags . \n",
    "If any of the cross correlations is statistically significantly different from 0 it will indicate a correlation between the two series.\n",
    "see: https://stats.stackexchange.com/questions/29096/correlation-between-two-time-series, https://stats.stackexchange.com/questions/26842/correlating-volume-timeseries,\n",
    "https://stackoverflow.com/questions/33171413/cross-correlation-time-lag-correlation-with-pandas#37215839"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crosscorr(y, X, lag=0):\n",
    "    \"\"\" Lag-N cross correlation. \n",
    "    Parameters\n",
    "    ----------\n",
    "    lag : int, default 0\n",
    "    y : pandas.Series object; independent variable\n",
    "    X : pandas.Series object; matrix of dependent variables\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    crosscorr : float\n",
    "    \"\"\"\n",
    "    return X.corr(y.shift(lag))    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hourly Data #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define data location\n",
    "#location = '/Users/mithras/Documents/_SCHOOL/_Drexel/BUSN 710 - Capstone/Data/Forecasting Project/'\n",
    "location = '/Users/loki/Documents/Data/Forecasting Project/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning & Checking "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hourly data\n",
    "use_jan_in = pd.read_excel(location+'Zip_HourlylUsage_2018.01.xlsx')\n",
    "use_feb_in = pd.read_excel(location+'PECO Zip HourlyUsage_2018.02.xlsx')\n",
    "use_mar_in = pd.read_excel(location+'PECO Zip HourlyUsage_2018.03.xlsx')\n",
    "use_apr_in = pd.read_excel(location+'PECO Zip HourlyUsage_2018.04.xlsx')\n",
    "use_may_in = pd.read_excel(location+'PECO Zip HourlyUsage_2018.05.xlsx')\n",
    "use_jun_in = pd.read_excel(location+'PECO Zip HourlyUsage_2018.06.xlsx')\n",
    "use_jul_in = pd.read_excel(location+'Zip_HourlylUsage_2018.07.xlsx')\n",
    "use_aug_in = pd.read_excel(location+'PECO Zip HourlyUsage_2018.08.xlsx')\n",
    "use_sep_in = pd.read_excel(location+'PECO Zip HourlyUsage_2018.09.xlsx')\n",
    "use_oct_in = pd.read_excel(location+'PECO Zip HourlyUsage_2017.10.xlsx')\n",
    "use_nov_in = pd.read_excel(location+'PECO Zip HourlyUsage_2017.11.xlsx') \n",
    "use_dec_in = pd.read_excel(location+'PECO Zip HourlyUsage_2017.12.xlsx') \n",
    "\n",
    "# other data\n",
    "customer_in = pd.read_excel(location+'PECO Zip Customer 2018.10.01 v2.xlsx', sheet_name=\"Account\")\n",
    "#ratecode_in = pd.read_excel(location+'Rate Codes for Drexel 9_28_2018.xlsx')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge use data\n",
    "hourly_in = [use_oct_in, use_nov_in, use_dec_in, use_jan_in, use_feb_in, use_mar_in, \n",
    "             use_apr_in, use_may_in, use_jun_in, use_jul_in, use_aug_in, use_sep_in]\n",
    "use = pd.concat(hourly_in)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create ID tuple\n",
    "ids = pd.Series(list(map(tuple, use[['DACCOUNTID', 'DMETERNO']].values)))\n",
    "use['ID'] = ids.values\n",
    "\n",
    "ids = pd.Series(list(map(tuple, customer_in[['DACCOUNTID', 'DMETERNO']].values)))\n",
    "customer_in['ID'] = ids.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find gas records\n",
    "use = use.loc[use['UOM'] == 'CCF']\n",
    "use = use.drop(columns=['UOM'])\n",
    "\n",
    "customer = customer_in.drop(columns=['CITY', 'STATE', 'ZIPCODE', 'COUNTYCODE'])\n",
    "\n",
    "# convert to datetime\n",
    "use['Dt'] =  pd.to_datetime(use['METERREADDATE'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Restructuring ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions for naming consistency\n",
    "def decrement(x, startswith, split):\n",
    "    \"\"\"\n",
    "    decrements a passed string of form \"demo#\" by 1\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x : string to be decremented\n",
    "    split : string to split on\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    y : decremented string\n",
    "    \"\"\"\n",
    "    if x.startswith(startswith):\n",
    "        a,b = x.split(split)\n",
    "        b = int(b)-1\n",
    "        y = a + split + str(b)\n",
    "\n",
    "        return y\n",
    "\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "    \n",
    "def interval_to_hour(df):\n",
    "    \"\"\"\n",
    "    function for fast rename/relabel during tidying process\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas data frame\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    df : data frame with updated column names\n",
    "    \"\"\"\n",
    "    \n",
    "    df = df.rename(columns=lambda x: decrement(x, \"INTERVAL_\", \"_\"))\n",
    "    df = df.rename(columns=lambda x: x.replace(\"INTERVAL_\", \"HR\"))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename for consistency\n",
    "use = interval_to_hour(use)\n",
    "use = use.drop(columns=['METERREADDATE','HR24'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tidy / Stack data (transform into tall data - one row per customer per hour):\n",
    "# ref: http://www.jeannicholashould.com/tidy-data-in-python.html\n",
    "tidy_use = pd.melt(use, \n",
    "                   id_vars=['ID','DACCOUNTID','DMETERNO','Dt'],\n",
    "                   var_name='Hour', value_name='Use')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# relabel/retype\n",
    "tidy_use['Hour'] = tidy_use['Hour'].str.extract('(\\d+)').astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append rate code data\n",
    "tidy_use = tidy_use.merge(customer, how='inner', on=['ID','DACCOUNTID','DMETERNO'])\n",
    "#tidy_use.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create single column for date and time\n",
    "tidy_use['year'] = pd.to_datetime(tidy_use['Dt'].values).year\n",
    "tidy_use['month'] = pd.to_datetime(tidy_use['Dt'].values).month\n",
    "tidy_use['day'] = pd.to_datetime(tidy_use['Dt'].values).day\n",
    "tidy_use['datetime'] = pd.to_datetime(tidy_use[['year','month','day','Hour']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add dummy variables for day-of-week\n",
    "#use = use.join(pd.get_dummies(use['Dt'].dt.weekday_name))\n",
    "tidy_use['Weekday'] = pd.get_dummies(tidy_use['Dt'].dt.weekday < 5)[True]\n",
    "\n",
    "# Add dummy variables for hour-of-day\n",
    "tidy_use = tidy_use.join(pd.get_dummies(tidy_use['Hour'],prefix='HR'))\n",
    "\n",
    "# Add dummy variables for holidays\n",
    "from pandas.tseries.holiday import USFederalHolidayCalendar\n",
    "cal = USFederalHolidayCalendar()\n",
    "dr = pd.date_range(start=min(tidy_use['Dt']), end=max(tidy_use['Dt']))\n",
    "holidays = cal.holidays(start=dr.min(), end=dr.max())\n",
    "\n",
    "tidy_use['Holiday'] = pd.get_dummies(tidy_use['Dt'].isin(holidays))[True]\n",
    "#tidy_use = tidy_use.rename(columns={True:'Holiday'})\n",
    "#tidy_use = tidy_use.drop(columns=[False])\n",
    "#tidy_use.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort by date & time\n",
    "tidy_use = tidy_use.drop(columns=['Dt','year','month','day','Hour'])\n",
    "tidy_use = tidy_use.sort_values(by=['datetime'])\n",
    "tidy_use = tidy_use.rename(columns={'datetime':'Dt'}) # for consistency\n",
    "#tidy_use.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quick look at amount of data we have per ID ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop records with missing data\n",
    "if len(tidy_use) == len(tidy_use.dropna(subset=['Use'])):\n",
    "    print(f'There is no missing data in the {len(use)} rows of useage data')\n",
    "else:\n",
    "    tidy_use = tidy_use.dropna(subset=['Use'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"There are {len(tidy_use['DACCOUNTID'].drop_duplicates())} unique AccountIDs in the data\")\n",
    "print(f\"There are {len(tidy_use['DMETERNO'].drop_duplicates())} unique MeterNos in the data\")\n",
    "ids = tidy_use['ID'].drop_duplicates()\n",
    "print(f\"There are {len(ids)} unique AccountID / MeterNo pairs in the data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "ids_to_plot = tidy_use.groupby(['ID']).count()\n",
    "sns.distplot(ids_to_plot['Use'], bins=20, kde=False, rug=False)\n",
    "plt.title('Frequency of IDs with X dates')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(ids_to_plot[ids_to_plot['Use'] > (7920)]['Use'], bins=50, kde=False, rug=False)\n",
    "plt.title('Frequency of IDs with at least 7920 hours (330 days)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find IDs with sufficient data to build initial segments on --> reliable training set ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find IDs with at least 360 days of data\n",
    "sufficient_ids = ids_to_plot[ids_to_plot['Use'] >= 8640].reset_index()['ID']\n",
    "print(f\"{100*len(sufficient_ids)/len(tidy_use['ID'].unique())} percent of IDs have at least 8640 hours (360 days) of data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the dates for which sufficient_ids all have data\n",
    "pivoted = (tidy_use[tidy_use['ID'].isin(sufficient_ids)]\n",
    "           .pivot_table(index=['Dt'], columns=['ID'], values='Use', fill_value=None))\n",
    "sufficient_dates = pivoted.dropna().reset_index()['Dt']\n",
    "print(f\"The IDs with sufficient data overlap on {len(sufficient_dates)} hours ({len(sufficient_dates)/24} days) of the most recent year of data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# filter total use dataset to contain only IDs who have sufficient data and only dates where sufficient_ids have data\n",
    "sufficient_use = tidy_use[(tidy_use['ID'].isin(sufficient_ids)) & (tidy_use['Dt'].isin(sufficient_dates))]\n",
    "#sufficient_use.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sanity check\n",
    "print(len(tidy_use))\n",
    "print(len(sufficient_use))\n",
    "print(len(ids))\n",
    "print(len(sufficient_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subgroup detection ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tidy_use[['ID','REVENUCODE']].drop_duplicates().groupby('REVENUCODE').count())\n",
    "print(\"nan           \"+str(len(tidy_use[np.isnan(tidy_use['REVENUCODE'])][['ID']].drop_duplicates())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tidy_use[['ID','REVENUCODE']].drop_duplicates().groupby('REVENUCODE').count())\n",
    "print(\"nan           \"+str(len(tidy_use[np.isnan(tidy_use['REVENUCODE'])][['ID']].drop_duplicates())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recode 'REVENUCODE' as 'TYPE'\n",
    "def recode(df):\n",
    "    \"\"\"combines REVENUCODE groups 1,12,nan to HOME and 3,5 to COMMERICAL\"\"\"\n",
    "    df['TYPE'] = None\n",
    "    df.loc[ df.REVENUCODE == 1, 'TYPE' ] = 'HOME'\n",
    "    df.loc[ df.REVENUCODE == 12, 'TYPE' ] = 'HOME'\n",
    "    df.loc[ np.isnan(df['REVENUCODE']), 'TYPE' ] = 'HOME'\n",
    "    df.loc[ df.REVENUCODE == 3, 'TYPE' ] = 'COMM'\n",
    "    df.loc[ df.REVENUCODE == 5, 'TYPE' ] = 'COMM'\n",
    "    \n",
    "    #df['TYPE'][df['REVENUCODE']==1] = 'HOME'\n",
    "    #df['TYPE'][df['REVENUCODE']==12] = 'HOME'\n",
    "    #df['TYPE'][np.isnan(df['REVENUCODE'])] = 'HOME'\n",
    "    #df['TYPE'][df['REVENUCODE']==3] = 'COMM'\n",
    "    #df['TYPE'][df['REVENUCODE']==5] = 'COMM'\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore errors\n",
    "tidy_use = recode(tidy_use)\n",
    "sufficient_use = recode(sufficient_use)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save file(s) as pickles\n",
    "# using save location to save with other data files outside of git repo\n",
    "# data.to_csv(location+'peco.csv', sep='\\t')\n",
    "tidy_use.to_pickle(location+'peco_hourly.pkl.zip')\n",
    "sufficient_use.to_pickle(location+'peco_sufficient_hourly.pkl.zip')\n",
    "ids.to_pickle(location+'peco_hourly_ids.pkl.zip')\n",
    "sufficient_ids.to_pickle(location+'peco_sufficient_hourly_ids.pkl.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Merge use & weather? ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "hourly = pd.read_pickle(location+'peco_hourly.pkl.zip')\n",
    "sufficient_hourly = pd.read_pickle(location+'peco_sufficient_hourly.pkl.zip')\n",
    "weather = pd.read_pickle(location+'hourly_weather.pkl.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge\n",
    "hourly = pd.merge(hourly, weather, how='inner', on=['Dt'])\n",
    "sufficient_hourly = pd.merge(sufficient_hourly, weather, how='inner', on=['Dt'])\n",
    "# CustIDs || Date | Consumption |||| Weather_variables "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
